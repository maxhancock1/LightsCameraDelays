{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取训练集和测试集的 CSV 文件\n",
    "ratings_train = pd.read_csv(\"ratings_train.csv\")\n",
    "ratings_test = pd.read_csv(\"ratings_test.csv\")\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "# 查看数据结构\n",
    "print(\"ratings_train 数据结构:\")\n",
    "print(ratings_train.info())  # 展示数据的总体结构\n",
    "\n",
    "\n",
    "print(\"\\nratings_test 数据结构:\")\n",
    "print(ratings_test.info())\n",
    "\n",
    "\n",
    "print(\"\\nmovies 数据结构:\")\n",
    "print(movies.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单查看数据\n",
    "print(movies.head())\n",
    "print(ratings_train.head())\n",
    "print(ratings_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 timestamp 列转换为时间戳（如果它是日期格式）\n",
    "ratings_train['timestamp'] = pd.to_datetime(ratings_train['timestamp'])\n",
    "ratings_train['timestamp'] = ratings_train['timestamp'].view('int64') / 10**9  # 转换为秒级时间戳\n",
    "\n",
    "# 计算皮尔逊相关系数\n",
    "correlation = ratings_train['timestamp'].corr(ratings_train['rating'])\n",
    "\n",
    "print(\"Timestamp 和 Rating 之间的相关度:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1 = pd.merge(ratings_train, movies, on='movieId', how='outer')\n",
    "merged_1.to_csv('merged_1.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2 = pd.merge(ratings_train, movies, on='movieId', how='inner')\n",
    "merged_2.to_csv('merged_2.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取 ratings_train 和 movies 数据\n",
    "ratings_train = pd.read_csv('ratings_train.csv', encoding='utf-8')\n",
    "movies = pd.read_csv('movies.csv', encoding='utf-8')\n",
    "\n",
    "# 将 ratings_train 中的 timestamp（float64，即 Unix 时间戳）转换为 datetime 格式\n",
    "ratings_train['timestamp'] = pd.to_datetime(ratings_train['timestamp'], unit='s')\n",
    "\n",
    "# 使用已经转换好的 timestamp 生成日期，并提取年月信息\n",
    "ratings_train['year_month'] = ratings_train['timestamp'].dt.to_period('M')\n",
    "\n",
    "# 检查 year_month 列\n",
    "print(\"Year-Month column:\")\n",
    "print(ratings_train[['timestamp', 'year_month']].head())\n",
    "\n",
    "# 拆分 genres 列，这里以字母 'I' 作为分隔符\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "\n",
    "# 展开 genres 列并清理数据\n",
    "movies_exploded = movies.explode('genre_list').rename(columns={'genre_list': 'genre'})\n",
    "\n",
    "# 检查拆分结果\n",
    "print(\"Exploded Movies DataFrame:\")\n",
    "print(movies_exploded[['movieId', 'genres', 'genre']].head())\n",
    "\n",
    "# 在 ratings_train 和 movies_exploded 中分别提取 movieId 对应的数据\n",
    "# 将 ratings_train 的每个 movieId 对应的 genre 从 movies_exploded 中获取\n",
    "df = pd.merge(ratings_train, movies_exploded[['movieId', 'genre']], on='movieId', how='inner')\n",
    "\n",
    "# 按每个月和每个 genre 分组，计算平均 rating\n",
    "monthly_avg = df.groupby(['year_month', 'genre'])['rating'].mean().reset_index()\n",
    "\n",
    "# 打印计算结果\n",
    "print(\"Monthly Average:\")\n",
    "print(monthly_avg.head())\n",
    "\n",
    "# 绘制每个 genre 的评分变化图\n",
    "if not monthly_avg.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for genre in monthly_avg['genre'].unique():\n",
    "        genre_data = monthly_avg[monthly_avg['genre'] == genre]\n",
    "        plt.plot(genre_data['year_month'].astype(str), genre_data['rating'], label=genre)\n",
    "\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.title('Average Rating per Genre over Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Genre')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 读取 ratings_train 和 movies 数据\n",
    "ratings_train = pd.read_csv('ratings_train.csv', encoding='utf-8')\n",
    "movies = pd.read_csv('movies.csv', encoding='utf-8')\n",
    "\n",
    "# 将 ratings_train 中的 timestamp 转换为 datetime 格式，并提取年份\n",
    "ratings_train['timestamp'] = pd.to_datetime(ratings_train['timestamp'], unit='s')\n",
    "ratings_train['year'] = ratings_train['timestamp'].dt.year\n",
    "\n",
    "# 拆分 movies 中的 genres 列，使用字母 'I' 作为分隔符\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "movies_exploded = movies.explode('genre_list').rename(columns={'genre_list': 'genre'})\n",
    "\n",
    "# 通过 movieId 将 ratings_train 和 movies_exploded 数据关联起来\n",
    "df = pd.merge(ratings_train, movies_exploded[['movieId', 'genre']], on='movieId', how='inner')\n",
    "\n",
    "# 按每年和每个 genre 分组，计算平均 rating\n",
    "yearly_avg = df.groupby(['year', 'genre'])['rating'].mean().reset_index()\n",
    "\n",
    "# 指定保存图片的路径\n",
    "save_path = r\"C:\\Users\\C\\Desktop\\SML\\Project 2\\TS\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# 针对每个 genre 单独生成图表\n",
    "for genre in yearly_avg['genre'].unique():\n",
    "    # 筛选当前 genre 的数据，并按年份排序\n",
    "    genre_data = yearly_avg[yearly_avg['genre'] == genre].sort_values('year')\n",
    "    \n",
    "    # 准备 x 轴：将年份转换为字符串\n",
    "    x_labels = genre_data['year'].astype(str)\n",
    "    x_positions = range(len(x_labels))\n",
    "    y_values = genre_data['rating']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # 绘制评分曲线\n",
    "    plt.plot(x_positions, y_values, marker='o', linestyle='-', label=genre)\n",
    "    \n",
    "    # 计算该 genre 的总平均 rating，并绘制水平基准线\n",
    "    overall_avg = df[df['genre'] == genre]['rating'].mean()\n",
    "    plt.axhline(y=overall_avg, color='red', linestyle='--', label=f'Overall Avg: {overall_avg:.2f}')\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.title(f'Average Rating Trend for {genre}')\n",
    "    plt.xticks(x_positions, x_labels, rotation=45)\n",
    "    plt.legend(title='Legend')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图像到指定目录，文件名为 \"{genre}_rating_trend.png\"\n",
    "    plt.savefig(os.path.join(save_path, f\"{genre}_rating_trend.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找特定记录\n",
    "user_records = merged_1[merged_1['movieId'] == 147426]\n",
    "print(user_records)\n",
    "\n",
    "# 查找符合多个条件的记录\n",
    "filtered_records = ratings_train[(ratings_train['userId'] == 5) & (ratings_train['movieId'] == 32)]\n",
    "print(filtered_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming ratings_train and ratings_test are pandas DataFrames\n",
    "known_users = ratings_train['userId'].unique()\n",
    "known_movies = ratings_train['movieId'].unique()\n",
    "\n",
    "test_users = ratings_test['userId'].unique()\n",
    "test_movies = ratings_test['movieId'].unique()\n",
    "\n",
    "cold_start_users = sum(~pd.Series(test_users).isin(known_users))\n",
    "cold_start_movies = sum(~pd.Series(test_movies).isin(known_movies))\n",
    "\n",
    "print(cold_start_users)  # Cold start users count\n",
    "print(cold_start_movies)  # Cold start movies count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载函数\n",
    "def load_data():\n",
    "    # 加载单个文件数据\n",
    "    data = pd.read_csv('merged_1.csv')  # 确保文件名称与实际一致\n",
    "    return data\n",
    "\n",
    "\n",
    "# 构建用户-电影评分矩阵\n",
    "def create_user_movie_matrix(data):\n",
    "    \"\"\"\n",
    "    从完整的数据构建用户-电影的评分矩阵\n",
    "    \"\"\"\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix.fillna(0, inplace=True)  # 将缺失值（未评分）填充为 0\n",
    "    return user_movie_matrix\n",
    "\n",
    "\n",
    "# 矩阵分解 (梯度下降法) 带早停机制\n",
    "def matrix_factorization_with_early_stopping(R, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    矩阵分解 (随机梯度下降) 带早停机制\n",
    "    参数:\n",
    "        R: 用户-电影评分矩阵\n",
    "        K: 潜在因子维度\n",
    "        steps: 最大迭代次数\n",
    "        alpha: 学习率\n",
    "        lambda_reg: 正则化参数\n",
    "        patience: 早停容忍次数\n",
    "        tolerance: 损失改善的最低阈值（提升小于该值时认为没有改善）\n",
    "    返回:\n",
    "        P: 用户特征矩阵\n",
    "        Q: 电影特征矩阵\n",
    "        best_loss: 最佳损失值\n",
    "    \"\"\"\n",
    "    num_users, num_movies = R.shape\n",
    "    P = np.random.rand(num_users, K)  # 初始化用户特征矩阵\n",
    "    Q = np.random.rand(num_movies, K)  # 初始化电影特征矩阵\n",
    "    \n",
    "    best_loss = float('inf')  # 最佳损失\n",
    "    patience_counter = 0  # 早停计数器\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # 随机梯度更新\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:  # 仅更新有评分的数据点\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "        \n",
    "        # 每步计算当前的总误差\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    loss += (R[i, j] - np.dot(P[i, :], Q[j, :].T)) ** 2\n",
    "        \n",
    "        # 打印每隔多步的损失\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "        \n",
    "        # 检查是否早停\n",
    "        if loss < best_loss - tolerance:  # 损失有显著改善\n",
    "            best_loss = loss\n",
    "            patience_counter = 0  # 重置早停计数器\n",
    "        else:  # 损失没有改善\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience {patience_counter}/{patience}: No significant improvement in loss.\")\n",
    "\n",
    "        if patience_counter >= patience:  # 提前停止\n",
    "            print(f\"Early stopping triggered. Best loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, best_loss\n",
    "\n",
    "\n",
    "# 获取预测评分\n",
    "def get_predictions(data, P, Q, user_id_mapping, movie_id_mapping):\n",
    "    \"\"\"\n",
    "    为每个用户预测评分，并将其调整为 0 到 5 且以 0.5 为间隔\n",
    "    \"\"\"\n",
    "    data['predicted_rating'] = data.apply(\n",
    "        lambda row: round_rating(\n",
    "            np.dot(P[user_id_mapping[row['userId']], :], Q[movie_id_mapping[row['movieId']], :].T)\n",
    "        ) if row['userId'] in user_id_mapping and row['movieId'] in movie_id_mapping else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "# 新增函数： 四舍五入到最近 0.5\n",
    "def round_rating(rating):\n",
    "    \"\"\"\n",
    "    将预测评分四舍五入到最近的 0.5 并裁剪到 [0, 5]\n",
    "    参数:\n",
    "        rating: 预测评分\n",
    "    返回:\n",
    "        调整后的评分\n",
    "    \"\"\"\n",
    "    rating = round(rating * 2) / 2  # 四舍五入到最近的 0.5\n",
    "    return np.clip(rating, 0, 5)  # 限制在 0 到 5 范围内\n",
    "\n",
    "# 替换主程序调用矩阵分解的部分\n",
    "def main1():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()  # 加载新数据集\n",
    "\n",
    "    print(\"\\nCreating user-movie matrix...\")\n",
    "    user_movie_matrix = create_user_movie_matrix(data)\n",
    "\n",
    "    # 获取用户和电影映射字典\n",
    "    user_id_mapping = {id: idx for idx, id in enumerate(user_movie_matrix.index)}\n",
    "    movie_id_mapping = {id: idx for idx, id in enumerate(user_movie_matrix.columns)}\n",
    "\n",
    "    print(\"Training Matrix Factorization Model...\")\n",
    "    latent_factors = 10  # 潜在因子数量\n",
    "    patience = 20  # 早停容忍次数\n",
    "    tolerance = 0.0001  # 损失改善的最低阈值\n",
    "    \n",
    "    P, Q, best_loss = matrix_factorization_with_early_stopping(\n",
    "        user_movie_matrix.to_numpy(), \n",
    "        K=latent_factors, \n",
    "        steps=1000, \n",
    "        alpha=0.002, \n",
    "        lambda_reg=0.1, \n",
    "        patience=patience, \n",
    "        tolerance=tolerance\n",
    "    )\n",
    "    print(f\"\\nMatrix Factorization Training Completed! Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "    # 保存结果\n",
    "    print(\"Saving trained matrices and mappings for testing...\")\n",
    "    np.save('P_matrix.npy', P)\n",
    "    np.save('Q_matrix.npy', Q)\n",
    "    np.save('user_id_mapping.npy', user_id_mapping)\n",
    "    np.save('movie_id_mapping.npy', movie_id_mapping)\n",
    "\n",
    "    # 计算全局平均评分并保存\n",
    "    global_avg_rating = np.mean(data['rating'])\n",
    "    np.save('global_avg_rating.npy', global_avg_rating)\n",
    "\n",
    "    print(\"Training data has been saved successfully.\")\n",
    "    \n",
    "    # print(\"Predicting User Ratings...\")\n",
    "    # # 新列包含预测评分\n",
    "    # data_with_predictions = get_predictions(data, P, Q, user_id_mapping, movie_id_mapping)\n",
    "\n",
    "    # # 保存结果到文件\n",
    "    # output_file = 'film_rating_predictions_group_E_week_Y.csv'\n",
    "    # data_with_predictions.to_csv(output_file, index=False)\n",
    "    # print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "    # # 计算均方误差（可选）\n",
    "    # mask = ~data_with_predictions['predicted_rating'].isna()  # 筛选出有效数据\n",
    "    # mse = mean_squared_error(data_with_predictions[mask]['rating'], data_with_predictions[mask]['predicted_rating'])\n",
    "    # print(f\"Mean Squared Error (MSE) on Train Data: {mse:.4f}\")\n",
    "\n",
    "main1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 数据加载函数\n",
    "def load_data():\n",
    "    # 加载单个文件数据\n",
    "    data = pd.read_csv('merged_1.csv')  # 确保文件名称与实际一致\n",
    "    return data\n",
    "\n",
    "\n",
    "# 构建用户-电影评分矩阵\n",
    "def create_user_movie_matrix(data):\n",
    "    \"\"\"\n",
    "    从完整的数据构建用户-电影的评分矩阵\n",
    "    \"\"\"\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix.fillna(0, inplace=True)  # 将缺失值（未评分）填充为 0\n",
    "    return user_movie_matrix\n",
    "\n",
    "\n",
    "# 矩阵分解 (梯度下降法) 带早停机制\n",
    "def matrix_factorization_with_early_stopping(R, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    矩阵分解 (随机梯度下降) 带早停机制\n",
    "    参数:\n",
    "        R: 用户-电影评分矩阵\n",
    "        K: 潜在因子维度\n",
    "        steps: 最大迭代次数\n",
    "        alpha: 学习率\n",
    "        lambda_reg: 正则化参数\n",
    "        patience: 早停容忍次数\n",
    "        tolerance: 损失改善的最低阈值（提升小于该值时认为没有改善）\n",
    "    返回:\n",
    "        P: 用户特征矩阵\n",
    "        Q: 电影特征矩阵\n",
    "        best_loss: 最佳损失值\n",
    "    \"\"\"\n",
    "    num_users, num_movies = R.shape\n",
    "    P = np.random.rand(num_users, K)  # 初始化用户特征矩阵\n",
    "    Q = np.random.rand(num_movies, K)  # 初始化电影特征矩阵\n",
    "    \n",
    "    best_loss = float('inf')  # 最佳损失\n",
    "    patience_counter = 0  # 早停计数器\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # 随机梯度更新\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:  # 仅更新有评分的数据点\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "        \n",
    "        # 每步计算当前的总误差\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    loss += (R[i, j] - np.dot(P[i, :], Q[j, :].T)) ** 2\n",
    "        \n",
    "        # 打印每隔多步的损失\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "        \n",
    "        # 检查是否早停\n",
    "        if loss < best_loss - tolerance:  # 损失有显著改善\n",
    "            best_loss = loss\n",
    "            patience_counter = 0  # 重置早停计数器\n",
    "        else:  # 损失没有改善\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience {patience_counter}/{patience}: No significant improvement in loss.\")\n",
    "\n",
    "        if patience_counter >= patience:  # 提前停止\n",
    "            print(f\"Early stopping triggered. Best loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, best_loss\n",
    "\n",
    "\n",
    "# 获取预测评分\n",
    "def get_predictions(data, P, Q, user_id_mapping, movie_id_mapping):\n",
    "    \"\"\"\n",
    "    为每个用户预测评分，并将其调整为 0 到 5 且以 0.5 为间隔\n",
    "    \"\"\"\n",
    "    data['predicted_rating'] = data.apply(\n",
    "        lambda row: round_rating(\n",
    "            np.dot(P[user_id_mapping[row['userId']], :], Q[movie_id_mapping[row['movieId']], :].T)\n",
    "        ) if row['userId'] in user_id_mapping and row['movieId'] in movie_id_mapping else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "# 新增函数： 四舍五入到最近 0.5\n",
    "def round_rating(rating):\n",
    "    \"\"\"\n",
    "    将预测评分四舍五入到最近的 0.5 并裁剪到 [0, 5]\n",
    "    参数:\n",
    "        rating: 预测评分\n",
    "    返回:\n",
    "        调整后的评分\n",
    "    \"\"\"\n",
    "    rating = round(rating * 2) / 2  # 四舍五入到最近的 0.5\n",
    "    return np.clip(rating, 0, 5)  # 限制在 0 到 5 范围内\n",
    "\n",
    "\n",
    "# 交叉验证并计算MSE\n",
    "def cross_validate(data, K=10, steps=100, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001, test_size=0.3, n_splits=5):\n",
    "    mse_list = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        print(f\"Cross-validation split {split + 1}/{n_splits}\")\n",
    "        \n",
    "        # 将数据划分为训练集和测试集\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size)\n",
    "\n",
    "        # 创建训练集的用户-电影评分矩阵\n",
    "        train_matrix = create_user_movie_matrix(train_data)\n",
    "\n",
    "        # 获取用户和电影映射字典\n",
    "        user_id_mapping = {id: idx for idx, id in enumerate(train_matrix.index)}\n",
    "        movie_id_mapping = {id: idx for idx, id in enumerate(train_matrix.columns)}\n",
    "\n",
    "        # 训练矩阵分解模型\n",
    "        P, Q, best_loss = matrix_factorization_with_early_stopping(\n",
    "            train_matrix.to_numpy(),\n",
    "            K=K,\n",
    "            steps=steps,\n",
    "            alpha=alpha,\n",
    "            lambda_reg=lambda_reg,\n",
    "            patience=patience,\n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        print(f\"Best Loss for this split: {best_loss:.4f}\")\n",
    "\n",
    "        # 计算测试集的预测评分\n",
    "        test_data_with_predictions = get_predictions(test_data, P, Q, user_id_mapping, movie_id_mapping)\n",
    "\n",
    "        # 计算均方误差 (MSE)\n",
    "        mask = ~test_data_with_predictions['predicted_rating'].isna()  # 筛选出有效的预测数据\n",
    "        mse = mean_squared_error(test_data_with_predictions[mask]['rating'], test_data_with_predictions[mask]['predicted_rating'])\n",
    "        print(f\"MSE for this split: {mse:.4f}\")\n",
    "        \n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    # 计算所有分割的平均MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"Average MSE across all splits: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "\n",
    "# 替换主程序调用交叉验证的部分\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()  # 加载数据集\n",
    "    avg_mse = cross_validate(data, n_splits=5)  # 5折交叉验证\n",
    "    print(f\"Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 四舍五入到最近的 0.5 且裁剪到 [0, 5]\n",
    "def round_rating(rating):\n",
    "    \"\"\"\n",
    "    四舍五入评分到最近的 0.5，然后裁剪到 [0, 5] 的范围\n",
    "    参数:\n",
    "        rating: 预测的评分值\n",
    "    返回:\n",
    "        四舍五入且裁剪后的评分\n",
    "    \"\"\"\n",
    "    rating = round(rating * 2) / 2  # 四舍五入到最近的 0.5\n",
    "    return np.clip(rating, 0, 5)  # 限制到 [0, 5]\n",
    "\n",
    "\n",
    "# 加载测试数据\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    加载测试数据集\n",
    "    参数:\n",
    "        file_path: 测试文件路径\n",
    "    返回:\n",
    "        测试数据 (dataframe)\n",
    "    \"\"\"\n",
    "    ratings_test = pd.read_csv(file_path, sep=',')  # 加载测试数据\n",
    "    return ratings_test\n",
    "\n",
    "\n",
    "# 预测测试集中评分\n",
    "def predict_test_ratings(test_data, P, Q, user_id_mapping, movie_id_mapping, global_avg_rating):\n",
    "    \"\"\"\n",
    "    预测测试数据集中的评分\n",
    "    参数:\n",
    "        test_data: 测试数据集 (dataframe)，包含 userId 和 movieId\n",
    "        P: 用户特征矩阵 (m x latent_factors)\n",
    "        Q: 电影特征矩阵 (n x latent_factors)\n",
    "        user_id_mapping: 用户 ID 映射到 P 的索引\n",
    "        movie_id_mapping: 电影 ID 映射到 Q 的索引\n",
    "        global_avg_rating: 训练集全局平均评分（作为冷启动策略时用）\n",
    "    返回:\n",
    "        测试数据集中预测的评分列表\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        \n",
    "        # 如果用户和电影在训练集中，使用矩阵分解结果预测评分\n",
    "        if user_id in user_id_mapping and movie_id in movie_id_mapping:\n",
    "            user_idx = user_id_mapping[user_id]\n",
    "            movie_idx = movie_id_mapping[movie_id]\n",
    "            predicted_rating = np.dot(P[user_idx, :], Q[movie_idx, :].T)\n",
    "        else:\n",
    "            # 冷启动策略：使用全局平均评分\n",
    "            predicted_rating = global_avg_rating\n",
    "        \n",
    "        # 四舍五入到 0.5 并裁剪到 [0, 5]\n",
    "        predictions.append(round_rating(predicted_rating))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main2():\n",
    "    print(\"Loading test data...\")\n",
    "    ratings_test = load_test_data('ratings_test.csv')  # 确保测试文件路径正确\n",
    "\n",
    "    print(\"Loading pre-trained model...\")\n",
    "    # 加载已保存的模型\n",
    "    try:\n",
    "        P = np.load('P_matrix.npy')\n",
    "        Q = np.load('Q_matrix.npy')\n",
    "        user_id_mapping = np.load('user_id_mapping.npy', allow_pickle=True).item()\n",
    "        movie_id_mapping = np.load('movie_id_mapping.npy', allow_pickle=True).item()\n",
    "        global_avg_rating = np.load('global_avg_rating.npy')  # 加载全局平均评分\n",
    "        print(\"Pre-trained model successfully loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model file missing: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Test Data Shape: {ratings_test.shape}\")\n",
    "    print(\"Predicting ratings on the test set...\")\n",
    "\n",
    "    # 调用预测函数\n",
    "    ratings_test['predicted_rating'] = predict_test_ratings(\n",
    "        ratings_test, P, Q, user_id_mapping, movie_id_mapping, global_avg_rating\n",
    "    )\n",
    "\n",
    "    # 保存预测结果到文件\n",
    "    output_file = 'film_rating_predictions_group_E_week_.csv'\n",
    "    ratings_test.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to '{output_file}'.\")\n",
    "\n",
    "    # （可选）评估模型性能：如果测试集有真实评分\n",
    "    if 'rating' in ratings_test.columns:\n",
    "        mse = mean_squared_error(ratings_test['rating'], ratings_test['predicted_rating'])\n",
    "        print(f\"Mean Squared Error (MSE) on Test Data: {mse:.4f}\")\n",
    "\n",
    "\n",
    "# 运行主函数\n",
    "main2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据加载函数\n",
    "def load_data():\n",
    "    # 加载单个文件数据\n",
    "    data = pd.read_csv('merged_1.csv')  # 确保文件名称与实际一致\n",
    "    return data\n",
    "\n",
    "# 构建用户-电影评分矩阵\n",
    "def create_user_movie_matrix(data):\n",
    "    \"\"\"\n",
    "    从完整的数据构建用户-电影的评分矩阵\n",
    "    \"\"\"\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix.dropna(inplace=True)\n",
    "\n",
    "    return user_movie_matrix\n",
    "\n",
    "\n",
    "# 将genres列转换为二进制矩阵\n",
    "def process_genres(data):\n",
    "    \"\"\"\n",
    "    将电影的genres信息转换为二进制向量。\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    return genres_df, mlb.classes_\n",
    "\n",
    "# 模型训练函数\n",
    "def matrix_factorization_with_genres(R, genres_matrix, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    矩阵分解 (随机梯度下降) 带早停机制，加入电影的genres信息\n",
    "    \"\"\"\n",
    "    num_users, num_movies = R.shape\n",
    "    num_genres = genres_matrix.shape[1]  # genres的维度\n",
    "    \n",
    "    P = np.random.rand(num_users, K)  # 用户特征矩阵\n",
    "    Q = np.random.rand(num_movies, K)  # 电影特征矩阵\n",
    "    G = np.random.rand(num_movies, num_genres)  # 电影的genres信息矩阵\n",
    "\n",
    "    best_loss = float('inf')  # 最佳损失\n",
    "    patience_counter = 0  # 早停计数器\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # 随机梯度更新\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:  # 仅更新有评分的数据点\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :].T) - np.dot(G[j, :], genres_matrix[j, :].T)\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "                    # 更新genres相关的G矩阵\n",
    "                    for g in range(num_genres):\n",
    "                        G[j, g] += alpha * (2 * eij * genres_matrix[j, g] - lambda_reg * G[j, g])\n",
    "\n",
    "        # 每步计算当前的总误差\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    loss += (R[i, j] - np.dot(P[i, :], Q[j, :].T) - np.dot(G[j, :], genres_matrix[j, :].T)) ** 2\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "\n",
    "        # 检查是否早停\n",
    "        if loss < best_loss - tolerance:  # 损失有显著改善\n",
    "            best_loss = loss\n",
    "            patience_counter = 0  # 重置早停计数器\n",
    "        else:  # 损失没有改善\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience {patience_counter}/{patience}: No significant improvement in loss.\")\n",
    "\n",
    "        if patience_counter >= patience:  # 提前停止\n",
    "            print(f\"Early stopping triggered. Best loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, G, best_loss\n",
    "\n",
    "# 计算预测评分\n",
    "def get_predictions(data, P, Q, G, user_id_mapping, movie_id_mapping, genres_matrix):\n",
    "    \"\"\"\n",
    "    为每个用户预测评分，并将其调整为 0 到 5 且以 0.5 为间隔\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['userId'] in user_id_mapping and row['movieId'] in movie_id_mapping:\n",
    "            user_idx = user_id_mapping[row['userId']]\n",
    "            movie_idx = movie_id_mapping[row['movieId']]\n",
    "            predicted_rating = np.dot(P[user_idx, :], Q[movie_idx, :].T) + np.dot(G[movie_idx, :], genres_matrix[movie_idx, :].T)\n",
    "            predictions.append(predicted_rating)\n",
    "        else:\n",
    "            predictions.append(np.nan)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 执行交叉验证并计算MSE\n",
    "def cross_validate(data, K=10, steps=100, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001, test_size=0.3, n_splits=5):\n",
    "    mse_list = []\n",
    "\n",
    "    for split in range(n_splits):\n",
    "        print(f\"Cross-validation split {split + 1}/{n_splits}\")\n",
    "        \n",
    "        # 将数据划分为训练集和测试集\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size)\n",
    "\n",
    "        # 创建训练集的用户-电影评分矩阵\n",
    "        train_matrix = create_user_movie_matrix(train_data)\n",
    "        genres_matrix, _ = process_genres(train_data)\n",
    "\n",
    "        # 获取用户和电影映射字典\n",
    "        user_id_mapping = {id: idx for idx, id in enumerate(train_matrix.index)}\n",
    "        movie_id_mapping = {id: idx for idx, id in enumerate(train_matrix.columns)}\n",
    "\n",
    "        # 训练矩阵分解模型\n",
    "        P, Q, G, best_loss = matrix_factorization_with_genres(\n",
    "            train_matrix.to_numpy(),\n",
    "            genres_matrix.to_numpy(),\n",
    "            K=K,\n",
    "            steps=steps,\n",
    "            alpha=alpha,\n",
    "            lambda_reg=lambda_reg,\n",
    "            patience=patience,\n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        print(f\"Best Loss for this split: {best_loss:.4f}\")\n",
    "\n",
    "        # 计算测试集的预测评分\n",
    "        test_data['predicted_rating'] = get_predictions(test_data, P, Q, G, user_id_mapping, movie_id_mapping, genres_matrix.to_numpy())\n",
    "\n",
    "        # 计算均方误差 (MSE)\n",
    "        mask = ~test_data['predicted_rating'].isna()  # 筛选出有效的预测数据\n",
    "        mse = mean_squared_error(test_data[mask]['rating'], test_data[mask]['predicted_rating'])\n",
    "        print(f\"MSE for this split: {mse:.4f}\")\n",
    "        \n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    # 计算所有分割的平均MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"Average MSE across all splits: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 调用交叉验证函数\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()  # 加载数据集\n",
    "    avg_mse = cross_validate(data, n_splits=5)  # 5折交叉验证\n",
    "    print(f\"Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "main()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv('merged_1.csv')\n",
    "    return data\n",
    "\n",
    "def process_genres(data):\n",
    "    unique_movies = data[['movieId', 'genres']].drop_duplicates().sort_values('movieId')\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(unique_movies['genres'].str.split('|'))\n",
    "    return genres_matrix, mlb.classes_\n",
    "\n",
    "def create_user_movie_matrix(data):\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix = user_movie_matrix.fillna(0)\n",
    "    return user_movie_matrix\n",
    "\n",
    "def svd_with_genres(R, genres_matrix, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    num_users, num_movies = R.shape\n",
    "    num_genres = genres_matrix.shape[1]\n",
    "    \n",
    "    P = np.random.rand(num_users, K)\n",
    "    Q = np.random.rand(num_movies, K)\n",
    "    G = np.random.rand(num_movies, num_genres)\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_movies)\n",
    "    valid_ratings = R[R > 0]\n",
    "    mu = np.mean(valid_ratings) if len(valid_ratings) > 0 else 0\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for step in range(steps):\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    prediction = mu + b_u[i] + b_i[j] + np.dot(P[i, :], Q[j, :].T) + np.dot(G[j, :], genres_matrix[j, :].T)\n",
    "                    eij = R[i, j] - prediction\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "                    for g in range(num_genres):\n",
    "                        G[j, g] += alpha * (2 * eij * genres_matrix[j, g] - lambda_reg * G[j, g])\n",
    "                    b_u[i] += alpha * (2 * eij - lambda_reg * b_u[i])\n",
    "                    b_i[j] += alpha * (2 * eij - lambda_reg * b_i[j])\n",
    "                    loss += eij ** 2\n",
    "        loss += lambda_reg * (np.sum(P**2) + np.sum(Q**2) + np.sum(G**2) + np.sum(b_u**2) + np.sum(b_i**2))\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "        \n",
    "        if loss < best_loss - tolerance:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at iteration {step}.\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, G, b_u, b_i, mu\n",
    "\n",
    "def get_predictions(R, P, Q, G, b_u, b_i, mu, genres_matrix, test_data, user_idx_dict, movie_idx_dict):\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        if user_id in user_idx_dict and movie_id in movie_idx_dict:\n",
    "            user_idx = user_idx_dict[user_id]\n",
    "            movie_idx = movie_idx_dict[movie_id]\n",
    "            prediction = (mu + b_u[user_idx] + b_i[movie_idx] + \n",
    "                         np.dot(P[user_idx, :], Q[movie_idx, :].T) + \n",
    "                         np.dot(G[movie_idx, :], genres_matrix[movie_idx, :].T))\n",
    "            if np.isnan(prediction) or np.isinf(prediction):\n",
    "                prediction = mu\n",
    "        else:\n",
    "            prediction = mu\n",
    "        predictions.append(prediction)\n",
    "    return np.array(predictions)\n",
    "\n",
    "def cross_validate(data, K=10, steps=100, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001, test_size=0.3, n_splits=5):\n",
    "    mse_list = []\n",
    "    for split in range(n_splits):\n",
    "        print(f\"Cross-validation split {split + 1}/{n_splits}\")\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size)\n",
    "        train_matrix = create_user_movie_matrix(train_data)\n",
    "        genres_matrix, _ = process_genres(train_data)\n",
    "        user_idx_dict = {user_id: idx for idx, user_id in enumerate(train_matrix.index)}\n",
    "        movie_idx_dict = {movie_id: idx for idx, movie_id in enumerate(train_matrix.columns)}\n",
    "        P, Q, G, b_u, b_i, mu = svd_with_genres(\n",
    "            train_matrix.to_numpy(), genres_matrix, K, steps, alpha, lambda_reg, patience, tolerance\n",
    "        )\n",
    "        predictions = get_predictions(\n",
    "            train_matrix.to_numpy(), P, Q, G, b_u, b_i, mu, genres_matrix, test_data, \n",
    "            user_idx_dict, movie_idx_dict\n",
    "        )\n",
    "        test_data['predicted_rating'] = predictions\n",
    "        valid_mask = ~np.isnan(test_data['rating'])\n",
    "        test_data_cleaned = test_data[valid_mask]\n",
    "        mse = mean_squared_error(test_data_cleaned['rating'], test_data_cleaned['predicted_rating'])\n",
    "        print(f\"MSE for this split: {mse:.4f}\")\n",
    "        mse_list.append(mse)\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"Average MSE across all splits: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()\n",
    "    avg_mse = cross_validate(data, n_splits=5)\n",
    "    print(f\"Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 假设你的训练数据文件名为 merged_1.csv / Assume your training data file is named merged_1.csv\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，并处理 genres 特征。\n",
    "    English: Convert timestamp to year and process genres feature.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为年份作为时间特征 / Convert timestamp to year as time feature\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到原始数据 / Merge genres_df to original data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb.classes_\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE（70% 训练，30% 测试）\n",
    "    English: Perform k-fold cross-validation and calculate MSE (70% training, 30% testing)\n",
    "    :param data: 预处理后的数据 / Preprocessed data\n",
    "    :param genre_cols: 类别列名 / Genre column names\n",
    "    :param k_fold: 折数 / Number of folds\n",
    "    \"\"\"\n",
    "    # 确保数据中没有 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    \n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # 创建映射字典 / Create mapping dictionaries\n",
    "        user_ids = train_data['userId'].unique()\n",
    "        movie_ids = train_data['movieId'].unique()\n",
    "        years = train_data['year'].unique()\n",
    "        \n",
    "        user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "        movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "        year_map = {y: idx for idx, y in enumerate(years)}\n",
    "        \n",
    "        # 初始化模型（高精度超参数） / Initialize model with high precision hyperparameters\n",
    "        model = SGDRecommender(\n",
    "            num_users=len(user_ids),\n",
    "            num_movies=len(movie_ids),\n",
    "            num_genres=len(genre_cols),\n",
    "            num_years=len(years),\n",
    "            k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "            alpha=0.005,  # 学习率 / Learning rate\n",
    "            lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "            max_iter=100    # 迭代次数 / Number of iterations\n",
    "        )\n",
    "        \n",
    "        # 训练模型 / Train the model\n",
    "        model.fit(train_data, user_map, movie_map, year_map, genre_cols)\n",
    "        \n",
    "        # 预测测试集 / Predict on test set\n",
    "        predictions = model.predict(test_data, user_map, movie_map, year_map, genre_cols)\n",
    "        \n",
    "        # 计算 MSE / Calculate MSE\n",
    "        mse = mean_squared_error(test_data['rating'], predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    # 输出平均 MSE / Output average MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 数据预处理 / Data preprocessing\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    # 执行交叉验证 / Perform cross-validation\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD2 Training Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 假设你的训练数据文件名为 merged_1.csv / Assume your training data file is named merged_1.csv\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，并处理 genres 特征。\n",
    "    English: Convert timestamp to year and process genres feature.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为年份作为时间特征 / Convert timestamp to year as time feature\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到原始数据 / Merge genres_df to original data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb.classes_\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 后处理预测评分 / Post-process predicted ratings\n",
    "def postprocess_predictions(predictions):\n",
    "    \"\"\"\n",
    "    中文：将预测评分裁剪到 0-5 并四舍五入到最近的 0.5 倍数。\n",
    "    English: Clip predictions to 0-5 and round to the nearest 0.5 multiple.\n",
    "    :param predictions: 原始预测评分 / Raw predicted ratings\n",
    "    :return: 处理后的预测评分 / Processed predicted ratings\n",
    "    \"\"\"\n",
    "    # 裁剪到 0-5 / Clip to 0-5\n",
    "    clipped_predictions = np.clip(predictions, 0, 5)\n",
    "    \n",
    "    # 四舍五入到最近的 0.5 倍数 / Round to nearest 0.5 multiple\n",
    "    rounded_predictions = np.round(clipped_predictions * 2) / 2\n",
    "    return rounded_predictions\n",
    "\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE（70% 训练，30% 测试）\n",
    "    English: Perform k-fold cross-validation and calculate MSE (70% training, 30% testing)\n",
    "    :param data: 预处理后的数据 / Preprocessed data\n",
    "    :param genre_cols: 类别列名 / Genre column names\n",
    "    :param k_fold: 折数 / Number of folds\n",
    "    \"\"\"\n",
    "    # 确保数据中没有 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    \n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # 创建映射字典 / Create mapping dictionaries\n",
    "        user_ids = train_data['userId'].unique()\n",
    "        movie_ids = train_data['movieId'].unique()\n",
    "        years = train_data['year'].unique()\n",
    "        \n",
    "        user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "        movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "        year_map = {y: idx for idx, y in enumerate(years)}\n",
    "        \n",
    "        # 初始化模型（高精度超参数） / Initialize model with high precision hyperparameters\n",
    "        model = SGDRecommender(\n",
    "            num_users=len(user_ids),\n",
    "            num_movies=len(movie_ids),\n",
    "            num_genres=len(genre_cols),\n",
    "            num_years=len(years),\n",
    "            k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "            alpha=0.005,  # 学习率 / Learning rate\n",
    "            lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "            max_iter=100    # 迭代次数 / Number of iterations\n",
    "        )\n",
    "        \n",
    "        # 训练模型 / Train the model\n",
    "        model.fit(train_data, user_map, movie_map, year_map, genre_cols)\n",
    "        \n",
    "        # 预测测试集 / Predict on test set\n",
    "        predictions = model.predict(test_data, user_map, movie_map, year_map, genre_cols)\n",
    "        \n",
    "        # 后处理预测评分 / Post-process predictions\n",
    "        predictions = postprocess_predictions(predictions)\n",
    "\n",
    "        # 计算 MSE / Calculate MSE\n",
    "        mse = mean_squared_error(test_data['rating'], predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    # 输出平均 MSE / Output average MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 数据预处理 / Data preprocessing\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    # 执行交叉验证 / Perform cross-validation\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD2 Training Pre + Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 假设您的训练数据文件名为 merged_1.csv\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，创建用户-电影评分矩阵并校正偏差。\n",
    "    English: Convert timestamp to year, process genres feature, create user-movie rating matrix and correct bias.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为 NaN 的行 / Remove rows where userId is NaN\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为年份作为时间特征 / Convert timestamp to year as time feature\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到原始数据 / Merge genres_df to original data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 创建用户-电影评分矩阵 / Create user-movie rating matrix\n",
    "    user_movie_matrix = data.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix = user_movie_matrix * 2\n",
    "    # 计算用户偏差和电影偏差 / Calculate user bias and movie bias\n",
    "    user_bias = data.groupby('userId')['rating'].mean()\n",
    "    movie_bias = data.groupby('movieId')['rating'].mean()\n",
    "    \n",
    "    # 校正评分矩阵 / Correct rating matrix\n",
    "    # 对齐 user_bias 和 user_movie_matrix 的行（用户） / Align user_bias with user_movie_matrix rows (users)\n",
    "    user_bias_aligned, _ = user_bias.align(user_movie_matrix, axis=0, join='right')\n",
    "    user_movie_matrix = user_movie_matrix.sub(user_bias_aligned, axis=0)\n",
    "    \n",
    "    # 对齐 movie_bias 和 user_movie_matrix 的列（电影） / Align movie_bias with user_movie_matrix columns (movies)\n",
    "    # 注意：Series.align 不能直接用于列对齐，所以我们对齐 movie_bias 和 user_movie_matrix.columns\n",
    "    movie_bias_aligned = movie_bias.reindex(user_movie_matrix.columns, fill_value=0)\n",
    "    user_movie_matrix = user_movie_matrix.sub(movie_bias_aligned, axis=1)\n",
    "    \n",
    "    # 处理 NaN 值 / Handle NaN values\n",
    "    user_movie_matrix = user_movie_matrix.fillna(0)\n",
    "    \n",
    "    return data, mlb.classes_, user_movie_matrix, user_bias, movie_bias\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差\n",
    "        self.mu = 0  # 全局均值\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols, user_bias, movie_bias):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分并进行后处理\n",
    "        English: Predict ratings for the test set and perform post-processing\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)\n",
    "            i = movie_map.get(row['movieId'], -1)\n",
    "            y = year_map.get(row['year'], -1)\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                \n",
    "                # 后处理：调整预测评分\n",
    "                if row['movieId'] in movie_bias:\n",
    "                    pred += user_bias.get(row['userId'], 0) + movie_bias.get(row['movieId'], 0)\n",
    "                    pred /= 2\n",
    "                else:\n",
    "                    pred += user_bias.get(row['userId'], 0)\n",
    "                pred = round(pred)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE\n",
    "    English: Perform k-fold cross-validation and calculate MSE\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        user_ids = train_data['userId'].unique()\n",
    "        movie_ids = train_data['movieId'].unique()\n",
    "        years = train_data['year'].unique()\n",
    "        \n",
    "        user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "        movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "        year_map = {y: idx for idx, y in enumerate(years)}\n",
    "        \n",
    "        user_bias = train_data.groupby('userId')['rating'].mean()\n",
    "        movie_bias = train_data.groupby('movieId')['rating'].mean()\n",
    "        \n",
    "        model = SGDRecommender(\n",
    "            num_users=len(user_ids),\n",
    "            num_movies=len(movie_ids),\n",
    "            num_genres=len(genre_cols),\n",
    "            num_years=len(years),\n",
    "            k=20,\n",
    "            alpha=0.005,\n",
    "            lambda_reg=0.1,\n",
    "            max_iter=100\n",
    "        )\n",
    "        \n",
    "        model.fit(train_data, user_map, movie_map, year_map, genre_cols)\n",
    "        predictions = model.predict(test_data, user_map, movie_map, year_map, genre_cols, user_bias, movie_bias)\n",
    "        \n",
    "        mse = mean_squared_error(test_data['rating'], predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    data_processed, genre_cols, user_movie_matrix, user_bias, movie_bias = preprocess_data(data)\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD2 Forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without rounding 最终选用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "train_data = pd.read_csv('merged_1.csv')  # 训练集文件 / Training set file\n",
    "test_data = pd.read_csv('ratings_test.csv')  # 测试集文件 / Test set file\n",
    "\n",
    "# 从训练集中提取 movieId 到 genres 的映射 / Extract movieId to genres mapping from training set\n",
    "movie_genres_map = train_data[['movieId', 'genres']].drop_duplicates().set_index('movieId')['genres'].to_dict()\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data, mlb=None, is_train=True, movie_genres_map=None):\n",
    "    \"\"\"\n",
    "    中文：对训练集或测试集进行预处理，转换 timestamp 和 genres。\n",
    "    English: Preprocess the training or test set by converting timestamp and genres.\n",
    "    :param data: 输入数据集 / Input dataset\n",
    "    :param mlb: MultiLabelBinarizer 对象，用于 genres 转换 / MultiLabelBinarizer object for genres transformation\n",
    "    :param is_train: 是否为训练集 / Whether it’s the training set\n",
    "    :param movie_genres_map: movieId 到 genres 的映射（测试集使用） / Mapping from movieId to genres (for test set)\n",
    "    :return: 预处理后的数据和 mlb（如果是训练集） / Preprocessed data and mlb (if training)\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为 nan 的行 / Remove rows where userId or movieId is nan\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "    \n",
    "    # 如果是测试集，添加 genres 列 / If test set, add genres column\n",
    "    if not is_train:\n",
    "        data = data.copy()  # 避免修改原始数据 / Avoid modifying original data\n",
    "        data['genres'] = data['movieId'].map(movie_genres_map)\n",
    "        # 处理缺失的 genres（movieId 不在训练集中） / Handle missing genres (movieId not in training set)\n",
    "        data['genres'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # 将 timestamp 转换为年份 / Convert timestamp to year\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    if is_train:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    else:\n",
    "        genres_matrix = mlb.transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到数据 / Merge genres_df to data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb if is_train else data\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "# # 后处理预测评分 / Post-process predicted ratings\n",
    "# def postprocess_predictions(predictions):\n",
    "#     \"\"\"\n",
    "#     中文：将预测评分裁剪到 0-5 并四舍五入到最近的 0.5 倍数。\n",
    "#     English: Clip predictions to 0-5 and round to the nearest 0.5 multiple.\n",
    "#     :param predictions: 原始预测评分 / Raw predicted ratings\n",
    "#     :return: 处理后的预测评分 / Processed predicted ratings\n",
    "#     \"\"\"\n",
    "#     # 裁剪到 0-5 / Clip to 0-5\n",
    "#     clipped_predictions = np.clip(predictions, 0, 5)\n",
    "    \n",
    "#     # 四舍五入到最近的 0.5 倍数 / Round to nearest 0.5 multiple\n",
    "#     rounded_predictions = np.round(clipped_predictions * 2) / 2\n",
    "#     return rounded_predictions\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 预处理训练集 / Preprocess training set\n",
    "    train_data_processed, mlb = preprocess_data(train_data, is_train=True)\n",
    "    \n",
    "    # 创建映射字典 / Create mapping dictionaries\n",
    "    user_ids = train_data_processed['userId'].unique()\n",
    "    movie_ids = train_data_processed['movieId'].unique()\n",
    "    years = train_data_processed['year'].unique()\n",
    "    genre_cols = mlb.classes_\n",
    "    \n",
    "    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "    year_map = {y: idx for idx, y in enumerate(years)}\n",
    "    \n",
    "    # 初始化并训练模型 / Initialize and train the model\n",
    "    model = SGDRecommender(\n",
    "        num_users=len(user_ids),\n",
    "        num_movies=len(movie_ids),\n",
    "        num_genres=len(genre_cols),\n",
    "        num_years=len(years),\n",
    "        k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "        alpha=0.005,  # 学习率 / Learning rate\n",
    "        lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "        max_iter=100    # 迭代次数 / Number of iterations\n",
    "    )\n",
    "    print(\"开始训练模型... / Starting model training...\")\n",
    "    model.fit(train_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 预处理测试集，传入 movie_genres_map / Preprocess test set with movie_genres_map\n",
    "    test_data_processed, _ = preprocess_data(test_data, mlb=mlb, is_train=False, movie_genres_map=movie_genres_map)\n",
    "    \n",
    "    # 预测测试集评分 / Predict ratings for test set\n",
    "    print(\"开始预测测试集... / Starting prediction on test set...\")\n",
    "    raw_predictions = model.predict(test_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # # 后处理预测评分 / Post-process predictions\n",
    "    # final_predictions = postprocess_predictions(raw_predictions)\n",
    "    \n",
    "    # 将处理后的预测结果添加到测试集 / Add processed predictions to test set\n",
    "    # test_data_processed['predicted_rating'] = final_predictions\n",
    "    test_data_processed['predicted_rating'] = raw_predictions\n",
    "    \n",
    "    # 输出预测结果 / Output prediction results\n",
    "    print(\"预测结果示例： / Prediction examples:\")\n",
    "    print(test_data_processed[['userId', 'movieId', 'timestamp', 'predicted_rating']].head())\n",
    "    \n",
    "    # 保存预测结果到文件 / Save predictions to file\n",
    "    test_data_processed.to_csv('predicted_ratings.csv', index=False)\n",
    "    print(\"预测结果已保存到 'predicted_ratings.csv' / Predictions saved to 'predicted_ratings.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "train_data = pd.read_csv('merged_1.csv')  # 训练集文件 / Training set file\n",
    "test_data = pd.read_csv('ratings_test.csv')  # 测试集文件 / Test set file\n",
    "\n",
    "# 从训练集中提取 movieId 到 genres 的映射 / Extract movieId to genres mapping from training set\n",
    "movie_genres_map = train_data[['movieId', 'genres']].drop_duplicates().set_index('movieId')['genres'].to_dict()\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data, mlb=None, is_train=True, movie_genres_map=None):\n",
    "    \"\"\"\n",
    "    中文：对训练集或测试集进行预处理，转换 timestamp 和 genres。\n",
    "    English: Preprocess the training or test set by converting timestamp and genres.\n",
    "    :param data: 输入数据集 / Input dataset\n",
    "    :param mlb: MultiLabelBinarizer 对象，用于 genres 转换 / MultiLabelBinarizer object for genres transformation\n",
    "    :param is_train: 是否为训练集 / Whether it’s the training set\n",
    "    :param movie_genres_map: movieId 到 genres 的映射（测试集使用） / Mapping from movieId to genres (for test set)\n",
    "    :return: 预处理后的数据和 mlb（如果是训练集） / Preprocessed data and mlb (if training)\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为 nan 的行 / Remove rows where userId or movieId is nan\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "    \n",
    "    # 如果是测试集，添加 genres 列 / If test set, add genres column\n",
    "    if not is_train:\n",
    "        data = data.copy()  # 避免修改原始数据 / Avoid modifying original data\n",
    "        data['genres'] = data['movieId'].map(movie_genres_map)\n",
    "        # 处理缺失的 genres（movieId 不在训练集中） / Handle missing genres (movieId not in training set)\n",
    "        data['genres'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # 将 timestamp 转换为年份 / Convert timestamp to year\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    if is_train:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    else:\n",
    "        genres_matrix = mlb.transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到数据 / Merge genres_df to data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb if is_train else data\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 后处理预测评分 / Post-process predicted ratings\n",
    "def postprocess_predictions(predictions):\n",
    "    \"\"\"\n",
    "    中文：将预测评分裁剪到 0-5 并四舍五入到最近的 0.5 倍数。\n",
    "    English: Clip predictions to 0-5 and round to the nearest 0.5 multiple.\n",
    "    :param predictions: 原始预测评分 / Raw predicted ratings\n",
    "    :return: 处理后的预测评分 / Processed predicted ratings\n",
    "    \"\"\"\n",
    "    # 裁剪到 0-5 / Clip to 0-5\n",
    "    clipped_predictions = np.clip(predictions, 0, 5)\n",
    "    \n",
    "    # 四舍五入到最近的 0.5 倍数 / Round to nearest 0.5 multiple\n",
    "    rounded_predictions = np.round(clipped_predictions * 2) / 2\n",
    "    return rounded_predictions\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 预处理训练集 / Preprocess training set\n",
    "    train_data_processed, mlb = preprocess_data(train_data, is_train=True)\n",
    "    \n",
    "    # 创建映射字典 / Create mapping dictionaries\n",
    "    user_ids = train_data_processed['userId'].unique()\n",
    "    movie_ids = train_data_processed['movieId'].unique()\n",
    "    years = train_data_processed['year'].unique()\n",
    "    genre_cols = mlb.classes_\n",
    "    \n",
    "    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "    year_map = {y: idx for idx, y in enumerate(years)}\n",
    "    \n",
    "    # 初始化并训练模型 / Initialize and train the model\n",
    "    model = SGDRecommender(\n",
    "        num_users=len(user_ids),\n",
    "        num_movies=len(movie_ids),\n",
    "        num_genres=len(genre_cols),\n",
    "        num_years=len(years),\n",
    "        k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "        alpha=0.005,  # 学习率 / Learning rate\n",
    "        lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "        max_iter=100    # 迭代次数 / Number of iterations\n",
    "    )\n",
    "    print(\"开始训练模型... / Starting model training...\")\n",
    "    model.fit(train_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 预处理测试集，传入 movie_genres_map / Preprocess test set with movie_genres_map\n",
    "    test_data_processed, _ = preprocess_data(test_data, mlb=mlb, is_train=False, movie_genres_map=movie_genres_map)\n",
    "    \n",
    "    # 预测测试集评分 / Predict ratings for test set\n",
    "    print(\"开始预测测试集... / Starting prediction on test set...\")\n",
    "    raw_predictions = model.predict(test_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 后处理预测评分 / Post-process predictions\n",
    "    final_predictions = postprocess_predictions(raw_predictions)\n",
    "    \n",
    "    # 将处理后的预测结果添加到测试集 / Add processed predictions to test set\n",
    "    test_data_processed['predicted_rating'] = final_predictions\n",
    "    \n",
    "    # 输出预测结果 / Output prediction results\n",
    "    print(\"预测结果示例： / Prediction examples:\")\n",
    "    print(test_data_processed[['userId', 'movieId', 'timestamp', 'predicted_rating']].head())\n",
    "    \n",
    "    # 保存预测结果到文件 / Save predictions to file\n",
    "    test_data_processed.to_csv('predicted_ratings.csv', index=False)\n",
    "    print(\"预测结果已保存到 'predicted_ratings.csv' / Predictions saved to 'predicted_ratings.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_7880\\2904777214.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\n",
      "正在处理第 1 折... / Processing fold 1...\n",
      "第 1 折 MSE: 0.6669 / Fold 1 MSE: 0.6669\n",
      "正在处理第 2 折... / Processing fold 2...\n",
      "第 2 折 MSE: 0.6537 / Fold 2 MSE: 0.6537\n",
      "正在处理第 3 折... / Processing fold 3...\n",
      "第 3 折 MSE: 0.6526 / Fold 3 MSE: 0.6526\n",
      "正在处理第 4 折... / Processing fold 4...\n",
      "第 4 折 MSE: 0.6439 / Fold 4 MSE: 0.6439\n",
      "正在处理第 5 折... / Processing fold 5...\n",
      "第 5 折 MSE: 0.6525 / Fold 5 MSE: 0.6525\n",
      "平均 MSE: 0.6539 / Average MSE: 0.6539\n",
      "最终平均 MSE: 0.6539 / Final Average MSE: 0.6539\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 假设您的训练数据文件名为 merged_1.csv\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，并添加用户和电影的平均评分特征。\n",
    "    English: Convert timestamp to year, process genres feature, and add mean rating features for users and movies.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为年份作为时间特征 / Convert timestamp to year as time feature\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到原始数据 / Merge genres_df to original data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 创建用户和电影的平均评分特征 / Create mean rating features for users and movies\n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    \n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE（70% 训练，30% 测试）\n",
    "    English: Perform k-fold cross-validation and calculate MSE (70% training, 30% testing)\n",
    "    \"\"\"\n",
    "    # 确保数据中没有 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    \n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # 特征列 / Feature columns\n",
    "        feature_cols = ['user_mean_rating', 'movie_mean_rating', 'year'] + list(genre_cols)\n",
    "        \n",
    "        # 训练集和测试集 / Training and test sets\n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['rating']\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data['rating']\n",
    "        \n",
    "        # 分割训练集为训练和验证集 / Split training set into train and validation\n",
    "        X_train_split = X_train[:-int(0.2 * len(X_train))]\n",
    "        X_val_split = X_train[-int(0.2 * len(X_train)):]\n",
    "        y_train_split = y_train[:-int(0.2 * len(y_train))]\n",
    "        y_val_split = y_train[-int(0.2 * len(y_train)):]\n",
    "        \n",
    "        # 创建DMatrix，用于XGBoost原生接口 / Create DMatrix for XGBoost native interface\n",
    "        dtrain = xgb.DMatrix(X_train_split, label=y_train_split)\n",
    "        dval = xgb.DMatrix(X_val_split, label=y_val_split)\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        \n",
    "        # 设置参数 / Define parameters\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',  # 平方误差回归\n",
    "            'eta': 0.05,                      # 学习率，降低以提高精度\n",
    "            'max_depth': 6,                   # 树的最大深度\n",
    "            'subsample': 0.8,                 # 使用80%样本，防止过拟合\n",
    "            'colsample_bytree': 0.8,          # 使用80%特征，防止过拟合\n",
    "            'nthread': -1,                    # 并行计算，使用所有CPU核心\n",
    "            'seed': 42                        # 随机种子，确保可复现\n",
    "        }\n",
    "        \n",
    "        # 训练模型，带早停 / Train with early stopping\n",
    "        evals = [(dval, 'eval')]  # 验证集用于早停\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=500,          # 最大树数量，支持早停\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=10,     # 10轮无改进则停止\n",
    "            verbose_eval=False            # 不显示训练过程\n",
    "        )\n",
    "        \n",
    "        # 预测测试集 / Predict on test set\n",
    "        predictions = model.predict(dtest)\n",
    "        \n",
    "        # 计算 MSE / Calculate MSE\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    # 输出平均 MSE / Output average MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 数据预处理 / Data preprocessing\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    # 执行交叉验证 / Perform cross-validation\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加模型特征：用户评分次数，单独用户平均评分，电影的年份,评分年份和电影年份的差。把全部特征用于冷启动问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering: the amount of user rating; the average of the user; the year of movie; the gap between rating year and movie year. Using all features for the cold start problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，提取电影年份，计算用户评分次数、用户平均评分、电影年份及评分年份与电影年份的差。\n",
    "    English: Convert timestamp to year, process genres feature, extract movie year, calculate user rating count, user mean rating, movie year, and the difference between rating year and movie year.\n",
    "    \"\"\"\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    data['movie_year'] = data['title'].apply(lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    user_rating_count = data.groupby('userId')['rating'].count().to_dict()\n",
    "    data['user_rating_count'] = data['userId'].map(user_rating_count)\n",
    "    \n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    \n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE（70% 训练，30% 测试）\n",
    "    English: Perform k-fold cross-validation and calculate MSE (70% training, 30% testing)\n",
    "    \"\"\"\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # 特征列 / Feature columns\n",
    "        feature_cols = ['user_rating_count', 'user_mean_rating', 'movie_mean_rating', 'rating_year', 'movie_year', 'year_diff'] + list(genre_cols)\n",
    "        \n",
    "        # 训练集和测试集 / Training and test sets\n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['rating']\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data['rating']\n",
    "        \n",
    "        # 分割训练集为训练和验证集 / Split training set into train and validation\n",
    "        X_train_split = X_train[:-int(0.2 * len(X_train))]\n",
    "        X_val_split = X_train[-int(0.2 * len(X_train)):]\n",
    "        y_train_split = y_train[:-int(0.2 * len(y_train))]\n",
    "        y_val_split = y_train[-int(0.2 * len(y_train)):]\n",
    "        \n",
    "        # 创建DMatrix / Create DMatrix for XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train_split, label=y_train_split)\n",
    "        dval = xgb.DMatrix(X_val_split, label=y_val_split)\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        \n",
    "        # 设置参数 / Define parameters\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eta': 0.05,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'nthread': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        # 训练模型，带早停 / Train with early stopping\n",
    "        evals = [(dval, 'eval')]\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=500,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=10,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # 预测测试集 / Predict on test set\n",
    "        predictions = model.predict(dtest)\n",
    "        \n",
    "        # 计算 MSE / Calculate MSE\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加特征：电影评分次数、用户评分方差。超参数调优：随机搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering: the amount of the movie rating; user rating variance.\n",
    "Optimal parameters: random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_26560\\377292504.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_26560\\377292504.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['title'].apply(lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\n",
      "正在处理第 1 折... / Processing fold 1...\n",
      "第 1 折 MSE: 0.6323 / Fold 1 MSE: 0.6323\n",
      "正在处理第 2 折... / Processing fold 2...\n",
      "第 2 折 MSE: 0.6195 / Fold 2 MSE: 0.6195\n",
      "正在处理第 3 折... / Processing fold 3...\n",
      "第 3 折 MSE: 0.6179 / Fold 3 MSE: 0.6179\n",
      "正在处理第 4 折... / Processing fold 4...\n",
      "第 4 折 MSE: 0.6146 / Fold 4 MSE: 0.6146\n",
      "正在处理第 5 折... / Processing fold 5...\n",
      "第 5 折 MSE: 0.6178 / Fold 5 MSE: 0.6178\n",
      "平均 MSE: 0.6204 / Average MSE: 0.6204\n",
      "最终平均 MSE: 0.6204 / Final Average MSE: 0.6204\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 替换为您的实际数据文件名\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，提取电影年份，计算用户评分次数、用户平均评分、用户评分方差、电影评分次数、电影年份及评分年份与电影年份的差。\n",
    "    English: Convert timestamp to year, process genres feature, extract movie year, calculate user rating count, user mean rating, user rating variance, movie rating count, movie year, and the difference between rating year and movie year.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为空的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为评分年份\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 从 title 中提取电影年份（如 \"Under Siege (1992)\"）\n",
    "    data['movie_year'] = data['title'].apply(lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan)\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 用户评分次数\n",
    "    user_rating_count = data.groupby('userId')['rating'].count().to_dict()\n",
    "    data['user_rating_count'] = data['userId'].map(user_rating_count)\n",
    "    \n",
    "    # 用户平均评分\n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    \n",
    "    # 用户评分方差\n",
    "    user_rating_var = data.groupby('userId')['rating'].var().to_dict()\n",
    "    data['user_rating_var'] = data['userId'].map(user_rating_var)\n",
    "    data['user_rating_var'] = data['user_rating_var'].fillna(0)  # 评分次数为1时方差为NaN，填充为0\n",
    "    \n",
    "    # 电影评分次数\n",
    "    movie_rating_count = data.groupby('movieId')['rating'].count().to_dict()\n",
    "    data['movie_rating_count'] = data['movieId'].map(movie_rating_count)\n",
    "    \n",
    "    # 电影平均评分\n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    # 评分年份与电影年份的差\n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 交叉验证与超参数调优 / Cross-validation with hyperparameter tuning\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并使用随机搜索进行超参数调优。\n",
    "    English: Perform k-fold cross-validation with randomized search for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # 特征列：使用所有特征\n",
    "        feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var', \n",
    "                        'movie_rating_count', 'movie_mean_rating', 'rating_year', \n",
    "                        'movie_year', 'year_diff'] + list(genre_cols)\n",
    "        \n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['rating']\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data['rating']\n",
    "        \n",
    "        # 定义超参数搜索空间\n",
    "        param_dist = {\n",
    "            'eta': [0.01, 0.03, 0.05, 0.1],           # 学习率 / Learning rate\n",
    "            'max_depth': [4, 6, 8, 10],               # 树的最大深度 / Max depth of trees\n",
    "            'subsample': [0.7, 0.8, 0.9],             # 样本采样率 / Subsample ratio\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9]       # 特征采样率 / Feature sampling ratio\n",
    "        }\n",
    "        \n",
    "        # XGBoost模型\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=500,\n",
    "            n_jobs=-1,  # 并行计算 / Parallel computation\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # 随机搜索\n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb_model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=20,  # 搜索20次组合 / 20 iterations\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=3,       # 内部3折交叉验证 / 3-fold CV\n",
    "            random_state=42\n",
    "        )\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        # 获取最佳模型\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # 预测测试集\n",
    "        predictions = best_model.predict(X_test)\n",
    "        \n",
    "        # 计算 MSE\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 数据预处理\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    # 执行交叉验证\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 3 Forecasting 最终选用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在预处理训练数据... / Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_7880\\4029847659.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_7880\\4029847659.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['title'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始超参数调优... / Starting hyperparameter tuning...\n",
      "超参数调优完成，最佳参数: / Hyperparameter tuning completed, best parameters: {'subsample': 0.7, 'max_depth': 8, 'eta': 0.03, 'colsample_bytree': 0.7}\n",
      "正在预处理测试数据... / Preprocessing test data...\n",
      "开始预测测试集... / Starting prediction on test set...\n",
      "预测结果示例： / Prediction examples:\n",
      "   userId  movieId   timestamp  predicted_rating\n",
      "0      73    49526  1255586478          3.283628\n",
      "1     187    47518  1237162935          3.561228\n",
      "2     150      788  1114306821          2.735262\n",
      "3     216     8830  1095792449          2.853003\n",
      "4     242     1227   956685476          4.811522\n",
      "预测结果已保存到 'predicted_ratings_xgb.csv' / Predictions saved to 'predicted_ratings_xgb.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "train_data = pd.read_csv('merged_1.csv')  # 训练集文件 / Training set file\n",
    "test_data = pd.read_csv('ratings_test.csv')  # 测试集文件 / Test set file\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，提取电影年份，计算用户评分次数、用户平均评分、用户评分方差、电影评分次数、电影年份及评分年份与电影年份的差。\n",
    "    English: Convert timestamp to year, process genres feature, extract movie year, calculate user rating count, user mean rating, user rating variance, movie rating count, movie year, and the difference between rating year and movie year.\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为空的行\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "\n",
    "    # 将 timestamp 转换为评分年份\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 从 title 中提取电影年份（如 \"Under Siege (1992)\"）\n",
    "    data['movie_year'] = data['title'].apply(\n",
    "        lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan\n",
    "    )\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 用户评分次数\n",
    "    user_rating_count = data.groupby('userId')['rating'].count().to_dict()\n",
    "    data['user_rating_count'] = data['userId'].map(user_rating_count)\n",
    "    \n",
    "    # 用户平均评分\n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    \n",
    "    # 用户评分方差\n",
    "    user_rating_var = data.groupby('userId')['rating'].var().to_dict()\n",
    "    data['user_rating_var'] = data['userId'].map(user_rating_var)\n",
    "    data['user_rating_var'] = data['user_rating_var'].fillna(0)  # 评分次数为1时方差为NaN，填充为0\n",
    "    \n",
    "    # 电影评分次数\n",
    "    movie_rating_count = data.groupby('movieId')['rating'].count().to_dict()\n",
    "    data['movie_rating_count'] = data['movieId'].map(movie_rating_count)\n",
    "    \n",
    "    # 电影平均评分\n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    # 评分年份与电影年份的差\n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 预处理训练集 / Preprocess training set\n",
    "    print(\"正在预处理训练数据... / Preprocessing training data...\")\n",
    "    train_data_processed, genre_cols = preprocess_data(train_data)\n",
    "    \n",
    "    # 计算用户和电影特征映射 / Compute user and movie feature mappings\n",
    "    user_features = train_data_processed.groupby('userId')[['user_rating_count', 'user_mean_rating', 'user_rating_var']].first().to_dict('index')\n",
    "    movie_features = train_data_processed.groupby('movieId')[['movie_rating_count', 'movie_mean_rating', 'movie_year'] + list(genre_cols)].first().to_dict('index')\n",
    "    \n",
    "    # 计算用于缺失值填充的统计值 / Compute statistics for imputation\n",
    "    impute_values = {\n",
    "        'user_rating_count': train_data_processed['user_rating_count'].median(),\n",
    "        'user_mean_rating': train_data_processed['user_mean_rating'].mean(),\n",
    "        'user_rating_var': train_data_processed['user_rating_var'].mean(),\n",
    "        'movie_rating_count': train_data_processed['movie_rating_count'].median(),\n",
    "        'movie_mean_rating': train_data_processed['movie_mean_rating'].mean(),\n",
    "        'movie_year': train_data_processed['movie_year'].median()\n",
    "    }\n",
    "    \n",
    "    # 定义特征列 / Define feature columns\n",
    "    feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var', \n",
    "                    'movie_rating_count', 'movie_mean_rating', 'rating_year', \n",
    "                    'movie_year', 'year_diff'] + list(genre_cols)\n",
    "    \n",
    "    # 准备训练数据 / Prepare training data\n",
    "    X_train = train_data_processed[feature_cols]\n",
    "    y_train = train_data_processed['rating']\n",
    "    \n",
    "    # 超参数调优 / Hyperparameter tuning\n",
    "    print(\"开始超参数调优... / Starting hyperparameter tuning...\")\n",
    "    param_dist = {\n",
    "        'eta': [0.01, 0.03, 0.05, 0.1],           # 学习率 / Learning rate\n",
    "        'max_depth': [4, 6, 8, 10],               # 树的最大深度 / Max depth of trees\n",
    "        'subsample': [0.7, 0.8, 0.9],             # 样本采样率 / Subsample ratio\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]       # 特征采样率 / Feature sampling ratio\n",
    "    }\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1,  # 并行计算 / Parallel computation\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # 搜索20次组合 / 20 iterations\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,       # 5折交叉验证 / 5-fold CV\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(\"超参数调优完成，最佳参数: / Hyperparameter tuning completed, best parameters:\", random_search.best_params_)\n",
    "    \n",
    "    # 加载并预处理测试集 / Load and preprocess test set\n",
    "    print(\"正在预处理测试数据... / Preprocessing test data...\")\n",
    "    test_data_processed = test_data.copy()\n",
    "    test_data_processed['rating_year'] = pd.to_datetime(test_data_processed['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 映射用户特征 / Map user features\n",
    "    for feature in ['user_rating_count', 'user_mean_rating', 'user_rating_var']:\n",
    "        test_data_processed[feature] = test_data_processed['userId'].map(\n",
    "            lambda x: user_features.get(x, {}).get(feature, impute_values[feature])\n",
    "        )\n",
    "    \n",
    "    # 映射电影特征 / Map movie features\n",
    "    for feature in ['movie_rating_count', 'movie_mean_rating', 'movie_year']:\n",
    "        test_data_processed[feature] = test_data_processed['movieId'].map(\n",
    "            lambda x: movie_features.get(x, {}).get(feature, impute_values[feature])\n",
    "        )\n",
    "    \n",
    "    # 映射 genres / Map genres\n",
    "    for genre in genre_cols:\n",
    "        test_data_processed[genre] = test_data_processed['movieId'].map(\n",
    "            lambda x: movie_features.get(x, {}).get(genre, 0)\n",
    "        )\n",
    "    \n",
    "    # 计算 year_diff / Compute year_diff\n",
    "    test_data_processed['year_diff'] = test_data_processed['rating_year'] - test_data_processed['movie_year']\n",
    "    \n",
    "    # 准备测试特征 / Prepare test features\n",
    "    X_test = test_data_processed[feature_cols]\n",
    "    \n",
    "    # 预测测试集评分 / Predict ratings for test set\n",
    "    print(\"开始预测测试集... / Starting prediction on test set...\")\n",
    "    predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # 将预测结果添加到测试集 / Add predictions to test set\n",
    "    test_data_processed['predicted_rating'] = predictions\n",
    "    \n",
    "    # 输出预测结果示例 / Output prediction examples\n",
    "    print(\"预测结果示例： / Prediction examples:\")\n",
    "    print(test_data_processed[['userId', 'movieId', 'timestamp', 'predicted_rating']].head())\n",
    "    \n",
    "    # 保存预测结果到文件 / Save predictions to file\n",
    "    test_data_processed.to_csv('predicted_ratings_xgb.csv', index=False)\n",
    "    print(\"预测结果已保存到 'predicted_ratings_xgb.csv' / Predictions saved to 'predicted_ratings_xgb.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征工程：\n",
    "1. 基于评分次数，使用归一化的用户和电影热门度分数\n",
    "2. 创建电影类型与年份或评分的交互项\n",
    "3. 计算用户近期评分与历史评分的差异，或电影近期评分趋势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_26560\\505709446.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_26560\\505709446.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['title'].apply(lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（70% 训练，30% 测试）...\n",
      "正在处理第 1 折...\n",
      "第 1 折 MSE: 0.6347\n",
      "正在处理第 2 折...\n",
      "第 2 折 MSE: 0.6218\n",
      "正在处理第 3 折...\n",
      "第 3 折 MSE: 0.6195\n",
      "正在处理第 4 折...\n",
      "第 4 折 MSE: 0.6169\n",
      "正在处理第 5 折...\n",
      "第 5 折 MSE: 0.6205\n",
      "平均 MSE: 0.6227\n",
      "最终平均 MSE: 0.6227\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载\n",
    "data = pd.read_csv('merged_1.csv')\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(data):\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    data['movie_year'] = data['title'].apply(lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    user_rating_count = data.groupby('userId')['rating'].count().to_dict()\n",
    "    data['user_rating_count'] = data['userId'].map(user_rating_count)\n",
    "    data['user_popularity'] = data['user_rating_count'] / data['user_rating_count'].max()\n",
    "    \n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    \n",
    "    user_rating_var = data.groupby('userId')['rating'].var().to_dict()\n",
    "    data['user_rating_var'] = data['userId'].map(user_rating_var).fillna(0)\n",
    "    \n",
    "    movie_rating_count = data.groupby('movieId')['rating'].count().to_dict()\n",
    "    data['movie_rating_count'] = data['movieId'].map(movie_rating_count)\n",
    "    data['movie_popularity'] = data['movie_rating_count'] / data['movie_rating_count'].max()\n",
    "    \n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "    \n",
    "    user_trend = data.groupby('userId').apply(lambda x: x['rating'].iloc[-1] - x['rating'].mean()).to_dict()\n",
    "    data['user_rating_trend'] = data['userId'].map(user_trend)\n",
    "\n",
    "    for genre in mlb.classes_:\n",
    "        data[f'{genre}_year'] = data[genre] * data['movie_year']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = ['user_rating_count', 'user_rating_var', 'movie_rating_count', 'year_diff']\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 交叉验证与超参数调优\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var', \n",
    "                        'movie_rating_count', 'movie_mean_rating', 'rating_year', \n",
    "                        'movie_year', 'year_diff', 'user_rating_trend', 'user_popularity', 'movie_popularity'] + \\\n",
    "                        [f'{g}_year' for g in genre_cols] + list(genre_cols)\n",
    "        \n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['rating']\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data['rating']\n",
    "        \n",
    "        param_dist = {\n",
    "            'eta': [0.005, 0.01, 0.03, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 6, 8, 10, 12],\n",
    "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'gamma': [0, 0.1, 0.5, 1]\n",
    "        }\n",
    "              \n",
    "        xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, n_jobs=-1, random_state=42)\n",
    "        random_search = RandomizedSearchCV(xgb_model, param_dist, n_iter=50, scoring='neg_mean_squared_error', cv=3, random_state=42)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = random_search.best_estimator_\n",
    "        predictions = best_model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序\n",
    "def main():\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在预处理训练数据... / Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\770274278.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\770274278.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['title'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（5折交叉验证）... / Starting cross-validation (5-fold cross-validation)...\n",
      "正在处理第 1 折... / Processing fold 1...\n",
      "第 1 折 MSE: 0.6316 / Fold 1 MSE: 0.6316\n",
      "正在处理第 2 折... / Processing fold 2...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 替换为您的实际训练集文件名\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，提取电影年份，计算用户评分次数、用户平均评分、用户评分方差、电影评分次数、电影年份及评分年份与电影年份的差。\n",
    "    English: Convert timestamp to year, process genres feature, extract movie year, calculate user rating count, user mean rating, user rating variance, movie rating count, movie year, and the difference between rating year and movie year.\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为空的行\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "\n",
    "    # 将 timestamp 转换为评分年份\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 从 title 中提取电影年份（如 \"Under Siege (1992)\"）\n",
    "    data['movie_year'] = data['title'].apply(\n",
    "        lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan\n",
    "    )\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 用户评分次数\n",
    "    user_rating_count = data.groupby('userId')['rating'].count().to_dict()\n",
    "    data['user_rating_count'] = data['userId'].map(user_rating_count)\n",
    "    \n",
    "    # 用户平均评分\n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    \n",
    "    # 用户评分方差\n",
    "    user_rating_var = data.groupby('userId')['rating'].var().to_dict()\n",
    "    data['user_rating_var'] = data['userId'].map(user_rating_var)\n",
    "    data['user_rating_var'] = data['user_rating_var'].fillna(0)  # 填充 NaN\n",
    "    \n",
    "    # 电影评分次数\n",
    "    movie_rating_count = data.groupby('movieId')['rating'].count().to_dict()\n",
    "    data['movie_rating_count'] = data['movieId'].map(movie_rating_count)\n",
    "    \n",
    "    # 电影平均评分\n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    # 评分年份与电影年份的差\n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 交叉验证与超参数调优 / Cross-validation with hyperparameter tuning\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    data = data.dropna(subset=['userId', 'movieId'])  # 再次确保无缺失值\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        val_data = data.iloc[val_idx]\n",
    "        \n",
    "        feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var', \n",
    "                        'movie_rating_count', 'movie_mean_rating', 'rating_year', \n",
    "                        'movie_year', 'year_diff'] + list(genre_cols)\n",
    "        \n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['rating']\n",
    "        X_val = val_data[feature_cols]\n",
    "        y_val = val_data['rating']\n",
    "        \n",
    "        param_dist = {\n",
    "            'eta': [0.01, 0.03, 0.05, 0.1],\n",
    "            'max_depth': [4, 6, 8, 10],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'reg_alpha': [0, 0.1, 1.0],\n",
    "            'reg_lambda': [0.1, 1.0, 10.0]\n",
    "        }\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=500,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb_model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=20,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        # 修改：移除 early_stopping_rounds 和 eval_set\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = random_search.best_estimator_\n",
    "        predictions = best_model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    print(\"正在预处理训练数据... / Preprocessing training data...\")\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    print(\"开始交叉验证（5折交叉验证）... / Starting cross-validation (5-fold cross-validation)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在预处理训练数据... / Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\203110301.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\203110301.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['title'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始超参数调优... / Starting hyperparameter tuning...\n",
      "超参数调优完成，最佳参数: / Hyperparameter tuning completed, best parameters: {'subsample': 0.8, 'reg_lambda': 1.0, 'reg_alpha': 0, 'max_depth': 8, 'eta': 0.03, 'colsample_bytree': 0.8}\n",
      "正在预处理测试数据... / Preprocessing test data...\n",
      "开始预测测试集... / Starting prediction on test set...\n",
      "预测结果示例： / Prediction examples:\n",
      "   userId  movieId   timestamp  predicted_rating\n",
      "0      73    49526  1255586478          3.282050\n",
      "1     187    47518  1237162935          3.495789\n",
      "2     150      788  1114306821          2.745424\n",
      "3     216     8830  1095792449          2.811216\n",
      "4     242     1227   956685476          4.825458\n",
      "预测结果已保存到 'predicted_ratings_xgb.csv' / Predictions saved to 'predicted_ratings_xgb.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "train_data = pd.read_csv('merged_1.csv')  # 训练集文件 / Training set file\n",
    "test_data = pd.read_csv('ratings_test.csv')  # 测试集文件 / Test set file\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，提取电影年份，计算用户评分次数、用户平均评分、用户评分方差、电影评分次数、电影年份及评分年份与电影年份的差。\n",
    "    English: Convert timestamp to year, process genres feature, extract movie year, calculate user rating count, user mean rating, user rating variance, movie rating count, movie year, and the difference between rating year and movie year.\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为空的行\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "\n",
    "    # 将 timestamp 转换为评分年份\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 从 title 中提取电影年份（如 \"Under Siege (1992)\"）\n",
    "    data['movie_year'] = data['title'].apply(\n",
    "        lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan\n",
    "    )\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 用户评分次数\n",
    "    user_rating_count = data.groupby('userId')['rating'].count().to_dict()\n",
    "    data['user_rating_count'] = data['userId'].map(user_rating_count)\n",
    "    \n",
    "    # 用户平均评分\n",
    "    user_mean_rating = data.groupby('userId')['rating'].mean().to_dict()\n",
    "    data['user_mean_rating'] = data['userId'].map(user_mean_rating)\n",
    "    \n",
    "    # 用户评分方差\n",
    "    user_rating_var = data.groupby('userId')['rating'].var().to_dict()\n",
    "    data['user_rating_var'] = data['userId'].map(user_rating_var)\n",
    "    data['user_rating_var'] = data['user_rating_var'].fillna(0)  # 评分次数为1时方差为NaN，填充为0\n",
    "    \n",
    "    # 电影评分次数\n",
    "    movie_rating_count = data.groupby('movieId')['rating'].count().to_dict()\n",
    "    data['movie_rating_count'] = data['movieId'].map(movie_rating_count)\n",
    "    \n",
    "    # 电影平均评分\n",
    "    movie_mean_rating = data.groupby('movieId')['rating'].mean().to_dict()\n",
    "    data['movie_mean_rating'] = data['movieId'].map(movie_mean_rating)\n",
    "    \n",
    "    # 评分年份与电影年份的差\n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "    \n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 预处理训练集 / Preprocess training set\n",
    "    print(\"正在预处理训练数据... / Preprocessing training data...\")\n",
    "    train_data_processed, genre_cols = preprocess_data(train_data)\n",
    "    \n",
    "    # 定义特征列 / Define feature columns\n",
    "    feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var', \n",
    "                    'movie_rating_count', 'movie_mean_rating', 'rating_year', \n",
    "                    'movie_year', 'year_diff'] + list(genre_cols)\n",
    "    \n",
    "    # 准备训练数据 / Prepare training data\n",
    "    X_train = train_data_processed[feature_cols]\n",
    "    y_train = train_data_processed['rating']\n",
    "    \n",
    "    # 超参数调优 / Hyperparameter tuning\n",
    "    print(\"开始超参数调优... / Starting hyperparameter tuning...\")\n",
    "    param_dist = {\n",
    "        'eta': [0.01, 0.03, 0.05, 0.1],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.1, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 10.0]\n",
    "    }\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=500,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    print(\"超参数调优完成，最佳参数: / Hyperparameter tuning completed, best parameters:\", random_search.best_params_)\n",
    "    \n",
    "    # 加载并预处理测试集 / Load and preprocess test set\n",
    "    print(\"正在预处理测试数据... / Preprocessing test data...\")\n",
    "    test_data_processed = test_data.copy()\n",
    "    test_data_processed['rating_year'] = pd.to_datetime(test_data_processed['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 映射用户特征 / Map user features\n",
    "    user_features = train_data_processed.groupby('userId')[['user_rating_count', 'user_mean_rating', 'user_rating_var']].first().to_dict('index')\n",
    "    for feature in ['user_rating_count', 'user_mean_rating', 'user_rating_var']:\n",
    "        test_data_processed[feature] = test_data_processed['userId'].map(\n",
    "            lambda x: user_features.get(x, {}).get(feature, train_data_processed[feature].median() if feature == 'user_rating_count' else train_data_processed[feature].mean())\n",
    "        )\n",
    "    \n",
    "    # 映射电影特征 / Map movie features\n",
    "    movie_features = train_data_processed.groupby('movieId')[['movie_rating_count', 'movie_mean_rating', 'movie_year'] + list(genre_cols)].first().to_dict('index')\n",
    "    for feature in ['movie_rating_count', 'movie_mean_rating', 'movie_year']:\n",
    "        test_data_processed[feature] = test_data_processed['movieId'].map(\n",
    "            lambda x: movie_features.get(x, {}).get(feature, train_data_processed[feature].median() if feature == 'movie_rating_count' else train_data_processed[feature].mean())\n",
    "        )\n",
    "    for genre in genre_cols:\n",
    "        test_data_processed[genre] = test_data_processed['movieId'].map(\n",
    "            lambda x: movie_features.get(x, {}).get(genre, 0)\n",
    "        )\n",
    "    \n",
    "    # 计算 year_diff / Compute year_diff\n",
    "    test_data_processed['year_diff'] = test_data_processed['rating_year'] - test_data_processed['movie_year']\n",
    "    \n",
    "    # 准备测试特征 / Prepare test features\n",
    "    X_test = test_data_processed[feature_cols]\n",
    "    \n",
    "    # 预测测试集评分 / Predict ratings for test set\n",
    "    print(\"开始预测测试集... / Starting prediction on test set...\")\n",
    "    predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # 将预测结果添加到测试集 / Add predictions to test set\n",
    "    test_data_processed['predicted_rating'] = predictions\n",
    "    \n",
    "    # 输出预测结果示例 / Output prediction examples\n",
    "    print(\"预测结果示例： / Prediction examples:\")\n",
    "    print(test_data_processed[['userId', 'movieId', 'timestamp', 'predicted_rating']].head())\n",
    "    \n",
    "    # 保存预测结果到文件 / Save predictions to file\n",
    "    test_data_processed.to_csv('predicted_ratings_xgb.csv', index=False)\n",
    "    print(\"预测结果已保存到 'predicted_ratings_xgb.csv' / Predictions saved to 'predicted_ratings_xgb.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regularization + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在预处理训练数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\2703140214.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\2703140214.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['title'].apply(\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\2703140214.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['movie_year'] = data['movie_year'].fillna(median_movie_year)\n",
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_12164\\2703140214.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['year_diff'] = data['rating_year'] - data['movie_year']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（5折交叉验证）...\n",
      "正在处理第 1 折...\n",
      "第 1 折保留的主成分数量: 24\n",
      "第 1 折 MSE: 0.6883\n",
      "正在处理第 2 折...\n",
      "第 2 折保留的主成分数量: 24\n",
      "第 2 折 MSE: 0.6691\n",
      "正在处理第 3 折...\n",
      "第 3 折保留的主成分数量: 24\n",
      "第 3 折 MSE: 0.6718\n",
      "正在处理第 4 折...\n",
      "第 4 折保留的主成分数量: 24\n",
      "第 4 折 MSE: 0.6616\n",
      "正在处理第 5 折...\n",
      "第 5 折保留的主成分数量: 24\n",
      "第 5 折 MSE: 0.6736\n",
      "平均 MSE: 0.6729\n",
      "最终平均 MSE: 0.6729\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 数据加载\n",
    "data = pd.read_csv('merged_1.csv')  # 替换为您的实际训练集文件名\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(data):\n",
    "    \"\"\"将原始数据转换为包含统计特征和genres二进制向量的格式，并处理NaN值\"\"\"\n",
    "    # 移除空值行\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "\n",
    "    # 时间特征\n",
    "    data['rating_year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    data['movie_year'] = data['title'].apply(\n",
    "        lambda x: int(re.search(r'\\((\\d{4})\\)', x).group(1)) if re.search(r'\\((\\d{4})\\)', x) else np.nan\n",
    "    )\n",
    "    # 填充 movie_year 的 NaN 为中位年份\n",
    "    median_movie_year = data['movie_year'].median()\n",
    "    data['movie_year'] = data['movie_year'].fillna(median_movie_year)\n",
    "    data['year_diff'] = data['rating_year'] - data['movie_year']\n",
    "\n",
    "    # 处理 genres\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "\n",
    "    # 用户统计特征\n",
    "    data['user_rating_count'] = data['userId'].map(data.groupby('userId')['rating'].count().to_dict())\n",
    "    data['user_mean_rating'] = data['userId'].map(data.groupby('userId')['rating'].mean().to_dict())\n",
    "    data['user_rating_var'] = data['userId'].map(data.groupby('userId')['rating'].var().to_dict())\n",
    "    data['user_rating_var'] = data['user_rating_var'].fillna(0)\n",
    "\n",
    "    # 电影统计特征\n",
    "    data['movie_rating_count'] = data['movieId'].map(data.groupby('movieId')['rating'].count().to_dict())\n",
    "    data['movie_mean_rating'] = data['movieId'].map(data.groupby('movieId')['rating'].mean().to_dict())\n",
    "\n",
    "    # 确保所有特征无 NaN\n",
    "    feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var',\n",
    "                    'movie_rating_count', 'movie_mean_rating', 'rating_year',\n",
    "                    'movie_year', 'year_diff'] + list(mlb.classes_)\n",
    "    data[feature_cols] = data[feature_cols].fillna(0)  # 填充任何剩余的 NaN\n",
    "\n",
    "    return data, mlb.classes_\n",
    "\n",
    "# 交叉验证与超参数调优\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"执行k折交叉验证，加入PCA降维\"\"\"\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "\n",
    "    # 特征列\n",
    "    feature_cols = ['user_rating_count', 'user_mean_rating', 'user_rating_var',\n",
    "                    'movie_rating_count', 'movie_mean_rating', 'rating_year',\n",
    "                    'movie_year', 'year_diff'] + list(genre_cols)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        val_data = data.iloc[val_idx]\n",
    "\n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['rating']\n",
    "        X_val = val_data[feature_cols]\n",
    "        y_val = val_data['rating']\n",
    "\n",
    "        # 标准化数据\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # 应用 PCA\n",
    "        pca = PCA(n_components=0.95)  # 保留95%的方差\n",
    "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "        X_val_pca = pca.transform(X_val_scaled)\n",
    "        print(f\"第 {fold+1} 折保留的主成分数量: {X_train_pca.shape[1]}\")\n",
    "\n",
    "        # 超参数搜索空间\n",
    "        param_dist = {\n",
    "            'eta': [0.01, 0.03, 0.05, 0.1],\n",
    "            'max_depth': [4, 6, 8, 10],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'reg_alpha': [0, 0.1, 1.0],\n",
    "            'reg_lambda': [0.1, 1.0, 10.0]\n",
    "        }\n",
    "\n",
    "        # XGBoost 模型\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=500,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # 随机搜索\n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb_model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=20,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "        # 获取最佳模型并预测\n",
    "        best_model = random_search.best_estimator_\n",
    "        predictions = best_model.predict(X_val_pca)\n",
    "        mse = mean_squared_error(y_val, predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f}\")\n",
    "\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序\n",
    "def main():\n",
    "    print(\"正在预处理训练数据...\")\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    print(\"开始交叉验证（5折交叉验证）...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
