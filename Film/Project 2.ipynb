{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取训练集和测试集的 CSV 文件\n",
    "ratings_train = pd.read_csv(\"ratings_train.csv\")\n",
    "ratings_test = pd.read_csv(\"ratings_test.csv\")\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "# 查看数据结构\n",
    "print(\"ratings_train 数据结构:\")\n",
    "print(ratings_train.info())  # 展示数据的总体结构\n",
    "\n",
    "\n",
    "print(\"\\nratings_test 数据结构:\")\n",
    "print(ratings_test.info())\n",
    "\n",
    "\n",
    "print(\"\\nmovies 数据结构:\")\n",
    "print(movies.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单查看数据\n",
    "print(movies.head())\n",
    "print(ratings_train.head())\n",
    "print(ratings_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 timestamp 列转换为时间戳（如果它是日期格式）\n",
    "ratings_train['timestamp'] = pd.to_datetime(ratings_train['timestamp'])\n",
    "ratings_train['timestamp'] = ratings_train['timestamp'].view('int64') / 10**9  # 转换为秒级时间戳\n",
    "\n",
    "# 计算皮尔逊相关系数\n",
    "correlation = ratings_train['timestamp'].corr(ratings_train['rating'])\n",
    "\n",
    "print(\"Timestamp 和 Rating 之间的相关度:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1 = pd.merge(ratings_train, movies, on='movieId', how='outer')\n",
    "merged_1.to_csv('merged_1.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2 = pd.merge(ratings_train, movies, on='movieId', how='inner')\n",
    "merged_2.to_csv('merged_2.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取 ratings_train 和 movies 数据\n",
    "ratings_train = pd.read_csv('ratings_train.csv', encoding='utf-8')\n",
    "movies = pd.read_csv('movies.csv', encoding='utf-8')\n",
    "\n",
    "# 将 ratings_train 中的 timestamp（float64，即 Unix 时间戳）转换为 datetime 格式\n",
    "ratings_train['timestamp'] = pd.to_datetime(ratings_train['timestamp'], unit='s')\n",
    "\n",
    "# 使用已经转换好的 timestamp 生成日期，并提取年月信息\n",
    "ratings_train['year_month'] = ratings_train['timestamp'].dt.to_period('M')\n",
    "\n",
    "# 检查 year_month 列\n",
    "print(\"Year-Month column:\")\n",
    "print(ratings_train[['timestamp', 'year_month']].head())\n",
    "\n",
    "# 拆分 genres 列，这里以字母 'I' 作为分隔符\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "\n",
    "# 展开 genres 列并清理数据\n",
    "movies_exploded = movies.explode('genre_list').rename(columns={'genre_list': 'genre'})\n",
    "\n",
    "# 检查拆分结果\n",
    "print(\"Exploded Movies DataFrame:\")\n",
    "print(movies_exploded[['movieId', 'genres', 'genre']].head())\n",
    "\n",
    "# 在 ratings_train 和 movies_exploded 中分别提取 movieId 对应的数据\n",
    "# 将 ratings_train 的每个 movieId 对应的 genre 从 movies_exploded 中获取\n",
    "df = pd.merge(ratings_train, movies_exploded[['movieId', 'genre']], on='movieId', how='inner')\n",
    "\n",
    "# 按每个月和每个 genre 分组，计算平均 rating\n",
    "monthly_avg = df.groupby(['year_month', 'genre'])['rating'].mean().reset_index()\n",
    "\n",
    "# 打印计算结果\n",
    "print(\"Monthly Average:\")\n",
    "print(monthly_avg.head())\n",
    "\n",
    "# 绘制每个 genre 的评分变化图\n",
    "if not monthly_avg.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for genre in monthly_avg['genre'].unique():\n",
    "        genre_data = monthly_avg[monthly_avg['genre'] == genre]\n",
    "        plt.plot(genre_data['year_month'].astype(str), genre_data['rating'], label=genre)\n",
    "\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.title('Average Rating per Genre over Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Genre')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 读取 ratings_train 和 movies 数据\n",
    "ratings_train = pd.read_csv('ratings_train.csv', encoding='utf-8')\n",
    "movies = pd.read_csv('movies.csv', encoding='utf-8')\n",
    "\n",
    "# 将 ratings_train 中的 timestamp 转换为 datetime 格式，并提取年份\n",
    "ratings_train['timestamp'] = pd.to_datetime(ratings_train['timestamp'], unit='s')\n",
    "ratings_train['year'] = ratings_train['timestamp'].dt.year\n",
    "\n",
    "# 拆分 movies 中的 genres 列，使用字母 'I' 作为分隔符\n",
    "movies['genre_list'] = movies['genres'].str.split('|')\n",
    "movies_exploded = movies.explode('genre_list').rename(columns={'genre_list': 'genre'})\n",
    "\n",
    "# 通过 movieId 将 ratings_train 和 movies_exploded 数据关联起来\n",
    "df = pd.merge(ratings_train, movies_exploded[['movieId', 'genre']], on='movieId', how='inner')\n",
    "\n",
    "# 按每年和每个 genre 分组，计算平均 rating\n",
    "yearly_avg = df.groupby(['year', 'genre'])['rating'].mean().reset_index()\n",
    "\n",
    "# 指定保存图片的路径\n",
    "save_path = r\"C:\\Users\\C\\Desktop\\SML\\Project 2\\TS\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# 针对每个 genre 单独生成图表\n",
    "for genre in yearly_avg['genre'].unique():\n",
    "    # 筛选当前 genre 的数据，并按年份排序\n",
    "    genre_data = yearly_avg[yearly_avg['genre'] == genre].sort_values('year')\n",
    "    \n",
    "    # 准备 x 轴：将年份转换为字符串\n",
    "    x_labels = genre_data['year'].astype(str)\n",
    "    x_positions = range(len(x_labels))\n",
    "    y_values = genre_data['rating']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # 绘制评分曲线\n",
    "    plt.plot(x_positions, y_values, marker='o', linestyle='-', label=genre)\n",
    "    \n",
    "    # 计算该 genre 的总平均 rating，并绘制水平基准线\n",
    "    overall_avg = df[df['genre'] == genre]['rating'].mean()\n",
    "    plt.axhline(y=overall_avg, color='red', linestyle='--', label=f'Overall Avg: {overall_avg:.2f}')\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.title(f'Average Rating Trend for {genre}')\n",
    "    plt.xticks(x_positions, x_labels, rotation=45)\n",
    "    plt.legend(title='Legend')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图像到指定目录，文件名为 \"{genre}_rating_trend.png\"\n",
    "    plt.savefig(os.path.join(save_path, f\"{genre}_rating_trend.png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找特定记录\n",
    "user_records = merged_1[merged_1['movieId'] == 147426]\n",
    "print(user_records)\n",
    "\n",
    "# 查找符合多个条件的记录\n",
    "filtered_records = ratings_train[(ratings_train['userId'] == 5) & (ratings_train['movieId'] == 32)]\n",
    "print(filtered_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming ratings_train and ratings_test are pandas DataFrames\n",
    "known_users = ratings_train['userId'].unique()\n",
    "known_movies = ratings_train['movieId'].unique()\n",
    "\n",
    "test_users = ratings_test['userId'].unique()\n",
    "test_movies = ratings_test['movieId'].unique()\n",
    "\n",
    "cold_start_users = sum(~pd.Series(test_users).isin(known_users))\n",
    "cold_start_movies = sum(~pd.Series(test_movies).isin(known_movies))\n",
    "\n",
    "print(cold_start_users)  # Cold start users count\n",
    "print(cold_start_movies)  # Cold start movies count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载函数\n",
    "def load_data():\n",
    "    # 加载单个文件数据\n",
    "    data = pd.read_csv('merged_1.csv')  # 确保文件名称与实际一致\n",
    "    return data\n",
    "\n",
    "\n",
    "# 构建用户-电影评分矩阵\n",
    "def create_user_movie_matrix(data):\n",
    "    \"\"\"\n",
    "    从完整的数据构建用户-电影的评分矩阵\n",
    "    \"\"\"\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix.fillna(0, inplace=True)  # 将缺失值（未评分）填充为 0\n",
    "    return user_movie_matrix\n",
    "\n",
    "\n",
    "# 矩阵分解 (梯度下降法) 带早停机制\n",
    "def matrix_factorization_with_early_stopping(R, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    矩阵分解 (随机梯度下降) 带早停机制\n",
    "    参数:\n",
    "        R: 用户-电影评分矩阵\n",
    "        K: 潜在因子维度\n",
    "        steps: 最大迭代次数\n",
    "        alpha: 学习率\n",
    "        lambda_reg: 正则化参数\n",
    "        patience: 早停容忍次数\n",
    "        tolerance: 损失改善的最低阈值（提升小于该值时认为没有改善）\n",
    "    返回:\n",
    "        P: 用户特征矩阵\n",
    "        Q: 电影特征矩阵\n",
    "        best_loss: 最佳损失值\n",
    "    \"\"\"\n",
    "    num_users, num_movies = R.shape\n",
    "    P = np.random.rand(num_users, K)  # 初始化用户特征矩阵\n",
    "    Q = np.random.rand(num_movies, K)  # 初始化电影特征矩阵\n",
    "    \n",
    "    best_loss = float('inf')  # 最佳损失\n",
    "    patience_counter = 0  # 早停计数器\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # 随机梯度更新\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:  # 仅更新有评分的数据点\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "        \n",
    "        # 每步计算当前的总误差\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    loss += (R[i, j] - np.dot(P[i, :], Q[j, :].T)) ** 2\n",
    "        \n",
    "        # 打印每隔多步的损失\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "        \n",
    "        # 检查是否早停\n",
    "        if loss < best_loss - tolerance:  # 损失有显著改善\n",
    "            best_loss = loss\n",
    "            patience_counter = 0  # 重置早停计数器\n",
    "        else:  # 损失没有改善\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience {patience_counter}/{patience}: No significant improvement in loss.\")\n",
    "\n",
    "        if patience_counter >= patience:  # 提前停止\n",
    "            print(f\"Early stopping triggered. Best loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, best_loss\n",
    "\n",
    "\n",
    "# 获取预测评分\n",
    "def get_predictions(data, P, Q, user_id_mapping, movie_id_mapping):\n",
    "    \"\"\"\n",
    "    为每个用户预测评分，并将其调整为 0 到 5 且以 0.5 为间隔\n",
    "    \"\"\"\n",
    "    data['predicted_rating'] = data.apply(\n",
    "        lambda row: round_rating(\n",
    "            np.dot(P[user_id_mapping[row['userId']], :], Q[movie_id_mapping[row['movieId']], :].T)\n",
    "        ) if row['userId'] in user_id_mapping and row['movieId'] in movie_id_mapping else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "# 新增函数： 四舍五入到最近 0.5\n",
    "def round_rating(rating):\n",
    "    \"\"\"\n",
    "    将预测评分四舍五入到最近的 0.5 并裁剪到 [0, 5]\n",
    "    参数:\n",
    "        rating: 预测评分\n",
    "    返回:\n",
    "        调整后的评分\n",
    "    \"\"\"\n",
    "    rating = round(rating * 2) / 2  # 四舍五入到最近的 0.5\n",
    "    return np.clip(rating, 0, 5)  # 限制在 0 到 5 范围内\n",
    "\n",
    "# 替换主程序调用矩阵分解的部分\n",
    "def main1():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()  # 加载新数据集\n",
    "\n",
    "    print(\"\\nCreating user-movie matrix...\")\n",
    "    user_movie_matrix = create_user_movie_matrix(data)\n",
    "\n",
    "    # 获取用户和电影映射字典\n",
    "    user_id_mapping = {id: idx for idx, id in enumerate(user_movie_matrix.index)}\n",
    "    movie_id_mapping = {id: idx for idx, id in enumerate(user_movie_matrix.columns)}\n",
    "\n",
    "    print(\"Training Matrix Factorization Model...\")\n",
    "    latent_factors = 10  # 潜在因子数量\n",
    "    patience = 20  # 早停容忍次数\n",
    "    tolerance = 0.0001  # 损失改善的最低阈值\n",
    "    \n",
    "    P, Q, best_loss = matrix_factorization_with_early_stopping(\n",
    "        user_movie_matrix.to_numpy(), \n",
    "        K=latent_factors, \n",
    "        steps=1000, \n",
    "        alpha=0.002, \n",
    "        lambda_reg=0.1, \n",
    "        patience=patience, \n",
    "        tolerance=tolerance\n",
    "    )\n",
    "    print(f\"\\nMatrix Factorization Training Completed! Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "    # 保存结果\n",
    "    print(\"Saving trained matrices and mappings for testing...\")\n",
    "    np.save('P_matrix.npy', P)\n",
    "    np.save('Q_matrix.npy', Q)\n",
    "    np.save('user_id_mapping.npy', user_id_mapping)\n",
    "    np.save('movie_id_mapping.npy', movie_id_mapping)\n",
    "\n",
    "    # 计算全局平均评分并保存\n",
    "    global_avg_rating = np.mean(data['rating'])\n",
    "    np.save('global_avg_rating.npy', global_avg_rating)\n",
    "\n",
    "    print(\"Training data has been saved successfully.\")\n",
    "    \n",
    "    # print(\"Predicting User Ratings...\")\n",
    "    # # 新列包含预测评分\n",
    "    # data_with_predictions = get_predictions(data, P, Q, user_id_mapping, movie_id_mapping)\n",
    "\n",
    "    # # 保存结果到文件\n",
    "    # output_file = 'film_rating_predictions_group_E_week_Y.csv'\n",
    "    # data_with_predictions.to_csv(output_file, index=False)\n",
    "    # print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "    # # 计算均方误差（可选）\n",
    "    # mask = ~data_with_predictions['predicted_rating'].isna()  # 筛选出有效数据\n",
    "    # mse = mean_squared_error(data_with_predictions[mask]['rating'], data_with_predictions[mask]['predicted_rating'])\n",
    "    # print(f\"Mean Squared Error (MSE) on Train Data: {mse:.4f}\")\n",
    "\n",
    "main1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 数据加载函数\n",
    "def load_data():\n",
    "    # 加载单个文件数据\n",
    "    data = pd.read_csv('merged_1.csv')  # 确保文件名称与实际一致\n",
    "    return data\n",
    "\n",
    "\n",
    "# 构建用户-电影评分矩阵\n",
    "def create_user_movie_matrix(data):\n",
    "    \"\"\"\n",
    "    从完整的数据构建用户-电影的评分矩阵\n",
    "    \"\"\"\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix.fillna(0, inplace=True)  # 将缺失值（未评分）填充为 0\n",
    "    return user_movie_matrix\n",
    "\n",
    "\n",
    "# 矩阵分解 (梯度下降法) 带早停机制\n",
    "def matrix_factorization_with_early_stopping(R, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    矩阵分解 (随机梯度下降) 带早停机制\n",
    "    参数:\n",
    "        R: 用户-电影评分矩阵\n",
    "        K: 潜在因子维度\n",
    "        steps: 最大迭代次数\n",
    "        alpha: 学习率\n",
    "        lambda_reg: 正则化参数\n",
    "        patience: 早停容忍次数\n",
    "        tolerance: 损失改善的最低阈值（提升小于该值时认为没有改善）\n",
    "    返回:\n",
    "        P: 用户特征矩阵\n",
    "        Q: 电影特征矩阵\n",
    "        best_loss: 最佳损失值\n",
    "    \"\"\"\n",
    "    num_users, num_movies = R.shape\n",
    "    P = np.random.rand(num_users, K)  # 初始化用户特征矩阵\n",
    "    Q = np.random.rand(num_movies, K)  # 初始化电影特征矩阵\n",
    "    \n",
    "    best_loss = float('inf')  # 最佳损失\n",
    "    patience_counter = 0  # 早停计数器\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # 随机梯度更新\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:  # 仅更新有评分的数据点\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :].T)\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "        \n",
    "        # 每步计算当前的总误差\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    loss += (R[i, j] - np.dot(P[i, :], Q[j, :].T)) ** 2\n",
    "        \n",
    "        # 打印每隔多步的损失\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "        \n",
    "        # 检查是否早停\n",
    "        if loss < best_loss - tolerance:  # 损失有显著改善\n",
    "            best_loss = loss\n",
    "            patience_counter = 0  # 重置早停计数器\n",
    "        else:  # 损失没有改善\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience {patience_counter}/{patience}: No significant improvement in loss.\")\n",
    "\n",
    "        if patience_counter >= patience:  # 提前停止\n",
    "            print(f\"Early stopping triggered. Best loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, best_loss\n",
    "\n",
    "\n",
    "# 获取预测评分\n",
    "def get_predictions(data, P, Q, user_id_mapping, movie_id_mapping):\n",
    "    \"\"\"\n",
    "    为每个用户预测评分，并将其调整为 0 到 5 且以 0.5 为间隔\n",
    "    \"\"\"\n",
    "    data['predicted_rating'] = data.apply(\n",
    "        lambda row: round_rating(\n",
    "            np.dot(P[user_id_mapping[row['userId']], :], Q[movie_id_mapping[row['movieId']], :].T)\n",
    "        ) if row['userId'] in user_id_mapping and row['movieId'] in movie_id_mapping else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "# 新增函数： 四舍五入到最近 0.5\n",
    "def round_rating(rating):\n",
    "    \"\"\"\n",
    "    将预测评分四舍五入到最近的 0.5 并裁剪到 [0, 5]\n",
    "    参数:\n",
    "        rating: 预测评分\n",
    "    返回:\n",
    "        调整后的评分\n",
    "    \"\"\"\n",
    "    rating = round(rating * 2) / 2  # 四舍五入到最近的 0.5\n",
    "    return np.clip(rating, 0, 5)  # 限制在 0 到 5 范围内\n",
    "\n",
    "\n",
    "# 交叉验证并计算MSE\n",
    "def cross_validate(data, K=10, steps=100, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001, test_size=0.3, n_splits=5):\n",
    "    mse_list = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        print(f\"Cross-validation split {split + 1}/{n_splits}\")\n",
    "        \n",
    "        # 将数据划分为训练集和测试集\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size)\n",
    "\n",
    "        # 创建训练集的用户-电影评分矩阵\n",
    "        train_matrix = create_user_movie_matrix(train_data)\n",
    "\n",
    "        # 获取用户和电影映射字典\n",
    "        user_id_mapping = {id: idx for idx, id in enumerate(train_matrix.index)}\n",
    "        movie_id_mapping = {id: idx for idx, id in enumerate(train_matrix.columns)}\n",
    "\n",
    "        # 训练矩阵分解模型\n",
    "        P, Q, best_loss = matrix_factorization_with_early_stopping(\n",
    "            train_matrix.to_numpy(),\n",
    "            K=K,\n",
    "            steps=steps,\n",
    "            alpha=alpha,\n",
    "            lambda_reg=lambda_reg,\n",
    "            patience=patience,\n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        print(f\"Best Loss for this split: {best_loss:.4f}\")\n",
    "\n",
    "        # 计算测试集的预测评分\n",
    "        test_data_with_predictions = get_predictions(test_data, P, Q, user_id_mapping, movie_id_mapping)\n",
    "\n",
    "        # 计算均方误差 (MSE)\n",
    "        mask = ~test_data_with_predictions['predicted_rating'].isna()  # 筛选出有效的预测数据\n",
    "        mse = mean_squared_error(test_data_with_predictions[mask]['rating'], test_data_with_predictions[mask]['predicted_rating'])\n",
    "        print(f\"MSE for this split: {mse:.4f}\")\n",
    "        \n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    # 计算所有分割的平均MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"Average MSE across all splits: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "\n",
    "# 替换主程序调用交叉验证的部分\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()  # 加载数据集\n",
    "    avg_mse = cross_validate(data, n_splits=5)  # 5折交叉验证\n",
    "    print(f\"Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 四舍五入到最近的 0.5 且裁剪到 [0, 5]\n",
    "def round_rating(rating):\n",
    "    \"\"\"\n",
    "    四舍五入评分到最近的 0.5，然后裁剪到 [0, 5] 的范围\n",
    "    参数:\n",
    "        rating: 预测的评分值\n",
    "    返回:\n",
    "        四舍五入且裁剪后的评分\n",
    "    \"\"\"\n",
    "    rating = round(rating * 2) / 2  # 四舍五入到最近的 0.5\n",
    "    return np.clip(rating, 0, 5)  # 限制到 [0, 5]\n",
    "\n",
    "\n",
    "# 加载测试数据\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    加载测试数据集\n",
    "    参数:\n",
    "        file_path: 测试文件路径\n",
    "    返回:\n",
    "        测试数据 (dataframe)\n",
    "    \"\"\"\n",
    "    ratings_test = pd.read_csv(file_path, sep=',')  # 加载测试数据\n",
    "    return ratings_test\n",
    "\n",
    "\n",
    "# 预测测试集中评分\n",
    "def predict_test_ratings(test_data, P, Q, user_id_mapping, movie_id_mapping, global_avg_rating):\n",
    "    \"\"\"\n",
    "    预测测试数据集中的评分\n",
    "    参数:\n",
    "        test_data: 测试数据集 (dataframe)，包含 userId 和 movieId\n",
    "        P: 用户特征矩阵 (m x latent_factors)\n",
    "        Q: 电影特征矩阵 (n x latent_factors)\n",
    "        user_id_mapping: 用户 ID 映射到 P 的索引\n",
    "        movie_id_mapping: 电影 ID 映射到 Q 的索引\n",
    "        global_avg_rating: 训练集全局平均评分（作为冷启动策略时用）\n",
    "    返回:\n",
    "        测试数据集中预测的评分列表\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        \n",
    "        # 如果用户和电影在训练集中，使用矩阵分解结果预测评分\n",
    "        if user_id in user_id_mapping and movie_id in movie_id_mapping:\n",
    "            user_idx = user_id_mapping[user_id]\n",
    "            movie_idx = movie_id_mapping[movie_id]\n",
    "            predicted_rating = np.dot(P[user_idx, :], Q[movie_idx, :].T)\n",
    "        else:\n",
    "            # 冷启动策略：使用全局平均评分\n",
    "            predicted_rating = global_avg_rating\n",
    "        \n",
    "        # 四舍五入到 0.5 并裁剪到 [0, 5]\n",
    "        predictions.append(round_rating(predicted_rating))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main2():\n",
    "    print(\"Loading test data...\")\n",
    "    ratings_test = load_test_data('ratings_test.csv')  # 确保测试文件路径正确\n",
    "\n",
    "    print(\"Loading pre-trained model...\")\n",
    "    # 加载已保存的模型\n",
    "    try:\n",
    "        P = np.load('P_matrix.npy')\n",
    "        Q = np.load('Q_matrix.npy')\n",
    "        user_id_mapping = np.load('user_id_mapping.npy', allow_pickle=True).item()\n",
    "        movie_id_mapping = np.load('movie_id_mapping.npy', allow_pickle=True).item()\n",
    "        global_avg_rating = np.load('global_avg_rating.npy')  # 加载全局平均评分\n",
    "        print(\"Pre-trained model successfully loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Model file missing: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Test Data Shape: {ratings_test.shape}\")\n",
    "    print(\"Predicting ratings on the test set...\")\n",
    "\n",
    "    # 调用预测函数\n",
    "    ratings_test['predicted_rating'] = predict_test_ratings(\n",
    "        ratings_test, P, Q, user_id_mapping, movie_id_mapping, global_avg_rating\n",
    "    )\n",
    "\n",
    "    # 保存预测结果到文件\n",
    "    output_file = 'film_rating_predictions_group_E_week_.csv'\n",
    "    ratings_test.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to '{output_file}'.\")\n",
    "\n",
    "    # （可选）评估模型性能：如果测试集有真实评分\n",
    "    if 'rating' in ratings_test.columns:\n",
    "        mse = mean_squared_error(ratings_test['rating'], ratings_test['predicted_rating'])\n",
    "        print(f\"Mean Squared Error (MSE) on Test Data: {mse:.4f}\")\n",
    "\n",
    "\n",
    "# 运行主函数\n",
    "main2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据加载函数\n",
    "def load_data():\n",
    "    # 加载单个文件数据\n",
    "    data = pd.read_csv('merged_1.csv')  # 确保文件名称与实际一致\n",
    "    return data\n",
    "\n",
    "# 构建用户-电影评分矩阵\n",
    "def create_user_movie_matrix(data):\n",
    "    \"\"\"\n",
    "    从完整的数据构建用户-电影的评分矩阵\n",
    "    \"\"\"\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix.dropna(inplace=True)\n",
    "\n",
    "    return user_movie_matrix\n",
    "\n",
    "\n",
    "# 将genres列转换为二进制矩阵\n",
    "def process_genres(data):\n",
    "    \"\"\"\n",
    "    将电影的genres信息转换为二进制向量。\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    return genres_df, mlb.classes_\n",
    "\n",
    "# 模型训练函数\n",
    "def matrix_factorization_with_genres(R, genres_matrix, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    \"\"\"\n",
    "    矩阵分解 (随机梯度下降) 带早停机制，加入电影的genres信息\n",
    "    \"\"\"\n",
    "    num_users, num_movies = R.shape\n",
    "    num_genres = genres_matrix.shape[1]  # genres的维度\n",
    "    \n",
    "    P = np.random.rand(num_users, K)  # 用户特征矩阵\n",
    "    Q = np.random.rand(num_movies, K)  # 电影特征矩阵\n",
    "    G = np.random.rand(num_movies, num_genres)  # 电影的genres信息矩阵\n",
    "\n",
    "    best_loss = float('inf')  # 最佳损失\n",
    "    patience_counter = 0  # 早停计数器\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # 随机梯度更新\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:  # 仅更新有评分的数据点\n",
    "                    eij = R[i, j] - np.dot(P[i, :], Q[j, :].T) - np.dot(G[j, :], genres_matrix[j, :].T)\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "                    # 更新genres相关的G矩阵\n",
    "                    for g in range(num_genres):\n",
    "                        G[j, g] += alpha * (2 * eij * genres_matrix[j, g] - lambda_reg * G[j, g])\n",
    "\n",
    "        # 每步计算当前的总误差\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    loss += (R[i, j] - np.dot(P[i, :], Q[j, :].T) - np.dot(G[j, :], genres_matrix[j, :].T)) ** 2\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "\n",
    "        # 检查是否早停\n",
    "        if loss < best_loss - tolerance:  # 损失有显著改善\n",
    "            best_loss = loss\n",
    "            patience_counter = 0  # 重置早停计数器\n",
    "        else:  # 损失没有改善\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience {patience_counter}/{patience}: No significant improvement in loss.\")\n",
    "\n",
    "        if patience_counter >= patience:  # 提前停止\n",
    "            print(f\"Early stopping triggered. Best loss: {best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, G, best_loss\n",
    "\n",
    "# 计算预测评分\n",
    "def get_predictions(data, P, Q, G, user_id_mapping, movie_id_mapping, genres_matrix):\n",
    "    \"\"\"\n",
    "    为每个用户预测评分，并将其调整为 0 到 5 且以 0.5 为间隔\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['userId'] in user_id_mapping and row['movieId'] in movie_id_mapping:\n",
    "            user_idx = user_id_mapping[row['userId']]\n",
    "            movie_idx = movie_id_mapping[row['movieId']]\n",
    "            predicted_rating = np.dot(P[user_idx, :], Q[movie_idx, :].T) + np.dot(G[movie_idx, :], genres_matrix[movie_idx, :].T)\n",
    "            predictions.append(predicted_rating)\n",
    "        else:\n",
    "            predictions.append(np.nan)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 执行交叉验证并计算MSE\n",
    "def cross_validate(data, K=10, steps=100, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001, test_size=0.3, n_splits=5):\n",
    "    mse_list = []\n",
    "\n",
    "    for split in range(n_splits):\n",
    "        print(f\"Cross-validation split {split + 1}/{n_splits}\")\n",
    "        \n",
    "        # 将数据划分为训练集和测试集\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size)\n",
    "\n",
    "        # 创建训练集的用户-电影评分矩阵\n",
    "        train_matrix = create_user_movie_matrix(train_data)\n",
    "        genres_matrix, _ = process_genres(train_data)\n",
    "\n",
    "        # 获取用户和电影映射字典\n",
    "        user_id_mapping = {id: idx for idx, id in enumerate(train_matrix.index)}\n",
    "        movie_id_mapping = {id: idx for idx, id in enumerate(train_matrix.columns)}\n",
    "\n",
    "        # 训练矩阵分解模型\n",
    "        P, Q, G, best_loss = matrix_factorization_with_genres(\n",
    "            train_matrix.to_numpy(),\n",
    "            genres_matrix.to_numpy(),\n",
    "            K=K,\n",
    "            steps=steps,\n",
    "            alpha=alpha,\n",
    "            lambda_reg=lambda_reg,\n",
    "            patience=patience,\n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        print(f\"Best Loss for this split: {best_loss:.4f}\")\n",
    "\n",
    "        # 计算测试集的预测评分\n",
    "        test_data['predicted_rating'] = get_predictions(test_data, P, Q, G, user_id_mapping, movie_id_mapping, genres_matrix.to_numpy())\n",
    "\n",
    "        # 计算均方误差 (MSE)\n",
    "        mask = ~test_data['predicted_rating'].isna()  # 筛选出有效的预测数据\n",
    "        mse = mean_squared_error(test_data[mask]['rating'], test_data[mask]['predicted_rating'])\n",
    "        print(f\"MSE for this split: {mse:.4f}\")\n",
    "        \n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    # 计算所有分割的平均MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"Average MSE across all splits: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 调用交叉验证函数\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()  # 加载数据集\n",
    "    avg_mse = cross_validate(data, n_splits=5)  # 5折交叉验证\n",
    "    print(f\"Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "main()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv('merged_1.csv')\n",
    "    return data\n",
    "\n",
    "def process_genres(data):\n",
    "    unique_movies = data[['movieId', 'genres']].drop_duplicates().sort_values('movieId')\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(unique_movies['genres'].str.split('|'))\n",
    "    return genres_matrix, mlb.classes_\n",
    "\n",
    "def create_user_movie_matrix(data):\n",
    "    user_movie_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
    "    user_movie_matrix = user_movie_matrix.fillna(0)\n",
    "    return user_movie_matrix\n",
    "\n",
    "def svd_with_genres(R, genres_matrix, K, steps, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001):\n",
    "    num_users, num_movies = R.shape\n",
    "    num_genres = genres_matrix.shape[1]\n",
    "    \n",
    "    P = np.random.rand(num_users, K)\n",
    "    Q = np.random.rand(num_movies, K)\n",
    "    G = np.random.rand(num_movies, num_genres)\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_movies)\n",
    "    valid_ratings = R[R > 0]\n",
    "    mu = np.mean(valid_ratings) if len(valid_ratings) > 0 else 0\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for step in range(steps):\n",
    "        loss = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_movies):\n",
    "                if R[i, j] > 0:\n",
    "                    prediction = mu + b_u[i] + b_i[j] + np.dot(P[i, :], Q[j, :].T) + np.dot(G[j, :], genres_matrix[j, :].T)\n",
    "                    eij = R[i, j] - prediction\n",
    "                    for k in range(K):\n",
    "                        P[i, k] += alpha * (2 * eij * Q[j, k] - lambda_reg * P[i, k])\n",
    "                        Q[j, k] += alpha * (2 * eij * P[i, k] - lambda_reg * Q[j, k])\n",
    "                    for g in range(num_genres):\n",
    "                        G[j, g] += alpha * (2 * eij * genres_matrix[j, g] - lambda_reg * G[j, g])\n",
    "                    b_u[i] += alpha * (2 * eij - lambda_reg * b_u[i])\n",
    "                    b_i[j] += alpha * (2 * eij - lambda_reg * b_i[j])\n",
    "                    loss += eij ** 2\n",
    "        loss += lambda_reg * (np.sum(P**2) + np.sum(Q**2) + np.sum(G**2) + np.sum(b_u**2) + np.sum(b_i**2))\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Iteration {step}/{steps} => Loss: {loss:.4f}\")\n",
    "        \n",
    "        if loss < best_loss - tolerance:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at iteration {step}.\")\n",
    "            break\n",
    "    \n",
    "    return P, Q, G, b_u, b_i, mu\n",
    "\n",
    "def get_predictions(R, P, Q, G, b_u, b_i, mu, genres_matrix, test_data, user_idx_dict, movie_idx_dict):\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        if user_id in user_idx_dict and movie_id in movie_idx_dict:\n",
    "            user_idx = user_idx_dict[user_id]\n",
    "            movie_idx = movie_idx_dict[movie_id]\n",
    "            prediction = (mu + b_u[user_idx] + b_i[movie_idx] + \n",
    "                         np.dot(P[user_idx, :], Q[movie_idx, :].T) + \n",
    "                         np.dot(G[movie_idx, :], genres_matrix[movie_idx, :].T))\n",
    "            if np.isnan(prediction) or np.isinf(prediction):\n",
    "                prediction = mu\n",
    "        else:\n",
    "            prediction = mu\n",
    "        predictions.append(prediction)\n",
    "    return np.array(predictions)\n",
    "\n",
    "def cross_validate(data, K=10, steps=100, alpha=0.002, lambda_reg=0.1, patience=10, tolerance=0.001, test_size=0.3, n_splits=5):\n",
    "    mse_list = []\n",
    "    for split in range(n_splits):\n",
    "        print(f\"Cross-validation split {split + 1}/{n_splits}\")\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size)\n",
    "        train_matrix = create_user_movie_matrix(train_data)\n",
    "        genres_matrix, _ = process_genres(train_data)\n",
    "        user_idx_dict = {user_id: idx for idx, user_id in enumerate(train_matrix.index)}\n",
    "        movie_idx_dict = {movie_id: idx for idx, movie_id in enumerate(train_matrix.columns)}\n",
    "        P, Q, G, b_u, b_i, mu = svd_with_genres(\n",
    "            train_matrix.to_numpy(), genres_matrix, K, steps, alpha, lambda_reg, patience, tolerance\n",
    "        )\n",
    "        predictions = get_predictions(\n",
    "            train_matrix.to_numpy(), P, Q, G, b_u, b_i, mu, genres_matrix, test_data, \n",
    "            user_idx_dict, movie_idx_dict\n",
    "        )\n",
    "        test_data['predicted_rating'] = predictions\n",
    "        valid_mask = ~np.isnan(test_data['rating'])\n",
    "        test_data_cleaned = test_data[valid_mask]\n",
    "        mse = mean_squared_error(test_data_cleaned['rating'], test_data_cleaned['predicted_rating'])\n",
    "        print(f\"MSE for this split: {mse:.4f}\")\n",
    "        mse_list.append(mse)\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"Average MSE across all splits: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()\n",
    "    avg_mse = cross_validate(data, n_splits=5)\n",
    "    print(f\"Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_14232\\4103148698.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\n",
      "正在处理第 1 折... / Processing fold 1...\n",
      "第 1 折 MSE: 0.8663 / Fold 1 MSE: 0.8663\n",
      "正在处理第 2 折... / Processing fold 2...\n",
      "第 2 折 MSE: 0.8486 / Fold 2 MSE: 0.8486\n",
      "正在处理第 3 折... / Processing fold 3...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 假设你的训练数据文件名为 merged_1.csv / Assume your training data file is named merged_1.csv\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，并处理 genres 特征。\n",
    "    English: Convert timestamp to year and process genres feature.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为年份作为时间特征 / Convert timestamp to year as time feature\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到原始数据 / Merge genres_df to original data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb.classes_\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE（70% 训练，30% 测试）\n",
    "    English: Perform k-fold cross-validation and calculate MSE (70% training, 30% testing)\n",
    "    :param data: 预处理后的数据 / Preprocessed data\n",
    "    :param genre_cols: 类别列名 / Genre column names\n",
    "    :param k_fold: 折数 / Number of folds\n",
    "    \"\"\"\n",
    "    # 确保数据中没有 userId 为 nan 的行\n",
    "    data = data.dropna(subset=['userId'])\n",
    "    \n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折... / Processing fold {fold+1}...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # 创建映射字典 / Create mapping dictionaries\n",
    "        user_ids = train_data['userId'].unique()\n",
    "        movie_ids = train_data['movieId'].unique()\n",
    "        years = train_data['year'].unique()\n",
    "        \n",
    "        user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "        movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "        year_map = {y: idx for idx, y in enumerate(years)}\n",
    "        \n",
    "        # 初始化模型（高精度超参数） / Initialize model with high precision hyperparameters\n",
    "        model = SGDRecommender(\n",
    "            num_users=len(user_ids),\n",
    "            num_movies=len(movie_ids),\n",
    "            num_genres=len(genre_cols),\n",
    "            num_years=len(years),\n",
    "            k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "            alpha=0.005,  # 学习率 / Learning rate\n",
    "            lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "            max_iter=100    # 迭代次数 / Number of iterations\n",
    "        )\n",
    "        \n",
    "        # 训练模型 / Train the model\n",
    "        model.fit(train_data, user_map, movie_map, year_map, genre_cols)\n",
    "        \n",
    "        # 预测测试集 / Predict on test set\n",
    "        predictions = model.predict(test_data, user_map, movie_map, year_map, genre_cols)\n",
    "        \n",
    "        # 计算 MSE / Calculate MSE\n",
    "        mse = mean_squared_error(test_data['rating'], predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f} / Fold {fold+1} MSE: {mse:.4f}\")\n",
    "    \n",
    "    # 输出平均 MSE / Output average MSE\n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f} / Average MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 数据预处理 / Data preprocessing\n",
    "    data_processed, genre_cols = preprocess_data(data)\n",
    "    \n",
    "    # 执行交叉验证 / Perform cross-validation\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）... / Starting cross-validation (70% training, 30% testing)...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f} / Final Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD2 Training Pre + Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C\\AppData\\Local\\Temp\\ipykernel_19276\\2507867835.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证（70% 训练，30% 测试）...\n",
      "正在处理第 1 折...\n",
      "第 1 折 MSE: 3.6556\n",
      "正在处理第 2 折...\n",
      "第 2 折 MSE: 3.6029\n",
      "正在处理第 3 折...\n",
      "第 3 折 MSE: 3.6655\n",
      "正在处理第 4 折...\n",
      "第 4 折 MSE: 3.5946\n",
      "正在处理第 5 折...\n",
      "第 5 折 MSE: 3.5841\n",
      "平均 MSE: 3.6205\n",
      "最终平均 MSE: 3.6205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "data = pd.read_csv('merged_1.csv')  # 假设您的训练数据文件名为 merged_1.csv\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    中文：将 timestamp 转换为年份，处理 genres 特征，创建用户-电影评分矩阵并校正偏差。\n",
    "    English: Convert timestamp to year, process genres feature, create user-movie rating matrix and correct bias.\n",
    "    \"\"\"\n",
    "    # 移除 userId 为 NaN 的行 / Remove rows where userId is NaN\n",
    "    data = data.dropna(subset=['userId'])\n",
    "\n",
    "    # 将 timestamp 转换为年份作为时间特征 / Convert timestamp to year as time feature\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到原始数据 / Merge genres_df to original data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    \n",
    "    # 创建用户-电影评分矩阵 / Create user-movie rating matrix\n",
    "    user_movie_matrix = data.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "    \n",
    "    # 计算用户偏差和电影偏差 / Calculate user bias and movie bias\n",
    "    user_bias = data.groupby('userId')['rating'].mean()\n",
    "    movie_bias = data.groupby('movieId')['rating'].mean()\n",
    "    \n",
    "    # 校正评分矩阵 / Correct rating matrix\n",
    "    # 对齐 user_bias 和 user_movie_matrix 的行（用户） / Align user_bias with user_movie_matrix rows (users)\n",
    "    user_bias_aligned, _ = user_bias.align(user_movie_matrix, axis=0, join='right')\n",
    "    user_movie_matrix = user_movie_matrix.sub(user_bias_aligned, axis=0)\n",
    "    \n",
    "    # 对齐 movie_bias 和 user_movie_matrix 的列（电影） / Align movie_bias with user_movie_matrix columns (movies)\n",
    "    # 注意：Series.align 不能直接用于列对齐，所以我们对齐 movie_bias 和 user_movie_matrix.columns\n",
    "    movie_bias_aligned = movie_bias.reindex(user_movie_matrix.columns, fill_value=0)\n",
    "    user_movie_matrix = user_movie_matrix.sub(movie_bias_aligned, axis=1)\n",
    "    \n",
    "    # 处理 NaN 值 / Handle NaN values\n",
    "    user_movie_matrix = user_movie_matrix.fillna(0)\n",
    "    \n",
    "    return data, mlb.classes_, user_movie_matrix, user_bias, movie_bias\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差\n",
    "        self.mu = 0  # 全局均值\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols, user_bias, movie_bias):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分并进行后处理\n",
    "        English: Predict ratings for the test set and perform post-processing\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)\n",
    "            i = movie_map.get(row['movieId'], -1)\n",
    "            y = year_map.get(row['year'], -1)\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                \n",
    "                # 后处理：调整预测评分\n",
    "                if row['movieId'] in movie_bias:\n",
    "                    pred += user_bias.get(row['userId'], 0) + movie_bias.get(row['movieId'], 0)\n",
    "                    pred /= 2\n",
    "                else:\n",
    "                    pred += user_bias.get(row['userId'], 0)\n",
    "                pred = round(pred)\n",
    "            \n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 交叉验证 / Cross-validation\n",
    "def cross_validate(data, genre_cols, k_fold=5):\n",
    "    \"\"\"\n",
    "    中文：执行 k 折交叉验证并计算 MSE\n",
    "    English: Perform k-fold cross-validation and calculate MSE\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    mse_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"正在处理第 {fold+1} 折...\")\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        user_ids = train_data['userId'].unique()\n",
    "        movie_ids = train_data['movieId'].unique()\n",
    "        years = train_data['year'].unique()\n",
    "        \n",
    "        user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "        movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "        year_map = {y: idx for idx, y in enumerate(years)}\n",
    "        \n",
    "        user_bias = train_data.groupby('userId')['rating'].mean()\n",
    "        movie_bias = train_data.groupby('movieId')['rating'].mean()\n",
    "        \n",
    "        model = SGDRecommender(\n",
    "            num_users=len(user_ids),\n",
    "            num_movies=len(movie_ids),\n",
    "            num_genres=len(genre_cols),\n",
    "            num_years=len(years),\n",
    "            k=20,\n",
    "            alpha=0.005,\n",
    "            lambda_reg=0.1,\n",
    "            max_iter=100\n",
    "        )\n",
    "        \n",
    "        model.fit(train_data, user_map, movie_map, year_map, genre_cols)\n",
    "        predictions = model.predict(test_data, user_map, movie_map, year_map, genre_cols, user_bias, movie_bias)\n",
    "        \n",
    "        mse = mean_squared_error(test_data['rating'], predictions)\n",
    "        mse_list.append(mse)\n",
    "        print(f\"第 {fold+1} 折 MSE: {mse:.4f}\")\n",
    "    \n",
    "    avg_mse = np.mean(mse_list)\n",
    "    print(f\"平均 MSE: {avg_mse:.4f}\")\n",
    "    return avg_mse\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    data_processed, genre_cols, user_movie_matrix, user_bias, movie_bias = preprocess_data(data)\n",
    "    print(\"开始交叉验证（70% 训练，30% 测试）...\")\n",
    "    avg_mse = cross_validate(data_processed, genre_cols, k_fold=5)\n",
    "    print(f\"最终平均 MSE: {avg_mse:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD2 Forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "train_data = pd.read_csv('merged_1.csv')  # 训练集文件 / Training set file\n",
    "test_data = pd.read_csv('ratings_test.csv')  # 测试集文件 / Test set file\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data, mlb=None, is_train=True):\n",
    "    \"\"\"\n",
    "    中文：对训练集或测试集进行预处理，转换 timestamp 和 genres。\n",
    "    English: Preprocess the training or test set by converting timestamp and genres.\n",
    "    :param data: 输入数据集 / Input dataset\n",
    "    :param mlb: MultiLabelBinarizer 对象，用于 genres 转换 / MultiLabelBinarizer object for genres transformation\n",
    "    :param is_train: 是否为训练集 / Whether it’s the training set\n",
    "    :return: 预处理后的数据和 mlb（如果是训练集） / Preprocessed data and mlb (if training)\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为 nan 的行 / Remove rows where userId or movieId is nan\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "    \n",
    "    # 将 timestamp 转换为年份 / Convert timestamp to year\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    if is_train:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    else:\n",
    "        genres_matrix = mlb.transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到数据 / Merge genres_df to data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb if is_train else data\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 预处理训练集 / Preprocess training set\n",
    "    train_data_processed, mlb = preprocess_data(train_data, is_train=True)\n",
    "    \n",
    "    # 创建映射字典 / Create mapping dictionaries\n",
    "    user_ids = train_data_processed['userId'].unique()\n",
    "    movie_ids = train_data_processed['movieId'].unique()\n",
    "    years = train_data_processed['year'].unique()\n",
    "    genre_cols = mlb.classes_\n",
    "    \n",
    "    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "    year_map = {y: idx for idx, y in enumerate(years)}\n",
    "    \n",
    "    # 初始化并训练模型 / Initialize and train the model\n",
    "    model = SGDRecommender(\n",
    "        num_users=len(user_ids),\n",
    "        num_movies=len(movie_ids),\n",
    "        num_genres=len(genre_cols),\n",
    "        num_years=len(years),\n",
    "        k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "        alpha=0.005,  # 学习率 / Learning rate\n",
    "        lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "        max_iter=100    # 迭代次数 / Number of iterations\n",
    "    )\n",
    "    print(\"开始训练模型... / Starting model training...\")\n",
    "    model.fit(train_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 预处理测试集 / Preprocess test set\n",
    "    test_data_processed, _ = preprocess_data(test_data, mlb=mlb, is_train=False)\n",
    "    \n",
    "    # 预测测试集评分 / Predict ratings for test set\n",
    "    print(\"开始预测测试集... / Starting prediction on test set...\")\n",
    "    predictions = model.predict(test_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 将预测结果添加到测试集 / Add predictions to test set\n",
    "    test_data_processed['predicted_rating'] = predictions\n",
    "    \n",
    "    # 输出预测结果 / Output prediction results\n",
    "    print(\"预测结果示例： / Prediction examples:\")\n",
    "    print(test_data_processed[['userId', 'movieId', 'timestamp', 'predicted_rating']].head())\n",
    "    \n",
    "    # 保存预测结果到文件 / Save predictions to file\n",
    "    test_data_processed.to_csv('predicted_ratings.csv', index=False)\n",
    "    print(\"预测结果已保存到 'predicted_ratings.csv' / Predictions saved to 'predicted_ratings.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 数据加载 / Data loading\n",
    "train_data = pd.read_csv('merged_1.csv')  # 训练集文件 / Training set file\n",
    "test_data = pd.read_csv('ratings_test.csv')  # 测试集文件 / Test set file\n",
    "\n",
    "# 数据预处理 / Data preprocessing\n",
    "def preprocess_data(data, mlb=None, is_train=True):\n",
    "    \"\"\"\n",
    "    中文：对训练集或测试集进行预处理，转换 timestamp 和 genres。\n",
    "    English: Preprocess the training or test set by converting timestamp and genres.\n",
    "    :param data: 输入数据集 / Input dataset\n",
    "    :param mlb: MultiLabelBinarizer 对象，用于 genres 转换 / MultiLabelBinarizer object for genres transformation\n",
    "    :param is_train: 是否为训练集 / Whether it’s the training set\n",
    "    :return: 预处理后的数据和 mlb（如果是训练集） / Preprocessed data and mlb (if training)\n",
    "    \"\"\"\n",
    "    # 移除 userId 或 movieId 为 nan 的行 / Remove rows where userId or movieId is nan\n",
    "    data = data.dropna(subset=['userId', 'movieId'])\n",
    "    \n",
    "    # 将 timestamp 转换为年份 / Convert timestamp to year\n",
    "    data['year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year\n",
    "    \n",
    "    # 处理 genres，转换为二进制向量 / Process genres, convert to binary vectors\n",
    "    if is_train:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        genres_matrix = mlb.fit_transform(data['genres'].str.split('|'))\n",
    "    else:\n",
    "        genres_matrix = mlb.transform(data['genres'].str.split('|'))\n",
    "    genres_df = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "    \n",
    "    # 合并 genres_df 到数据 / Merge genres_df to data\n",
    "    data = pd.concat([data, genres_df], axis=1)\n",
    "    return data, mlb if is_train else data\n",
    "\n",
    "# SGD 推荐模型 / SGD Recommender Model\n",
    "class SGDRecommender:\n",
    "    def __init__(self, num_users, num_movies, num_genres, num_years, k=20, alpha=0.005, lambda_reg=0.1, max_iter=100):\n",
    "        \"\"\"\n",
    "        中文：初始化模型参数\n",
    "        English: Initialize model parameters\n",
    "        :param num_users: 用户数量 / Number of users\n",
    "        :param num_movies: 电影数量 / Number of movies\n",
    "        :param num_genres: 类别数量 / Number of genres\n",
    "        :param num_years: 年份数量 / Number of years\n",
    "        :param k: 潜在特征维度 / Latent feature dimension\n",
    "        :param alpha: 学习率 / Learning rate\n",
    "        :param lambda_reg: 正则化参数 / Regularization parameter\n",
    "        :param max_iter: 最大迭代次数 / Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.num_genres = num_genres\n",
    "        self.num_years = num_years\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # 初始化参数 / Initialize parameters\n",
    "        self.P = np.random.normal(scale=1./k, size=(num_users, k))  # 用户潜在特征 / User latent features\n",
    "        self.Q = np.random.normal(scale=1./k, size=(num_movies, k))  # 电影潜在特征 / Movie latent features\n",
    "        self.b_u = np.zeros(num_users)  # 用户偏差 / User bias\n",
    "        self.b_i = np.zeros(num_movies)  # 电影偏差 / Movie bias\n",
    "        self.b_g = np.zeros(num_genres)  # 类别偏差 / Genre bias\n",
    "        self.b_y = np.zeros(num_years)  # 年份偏差 / Year bias\n",
    "        self.mu = 0  # 全局均值 / Global mean\n",
    "    \n",
    "    def fit(self, train_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：训练模型\n",
    "        English: Train the model\n",
    "        \"\"\"\n",
    "        # 计算全局均值 / Calculate global mean\n",
    "        self.mu = train_data['rating'].mean()\n",
    "        \n",
    "        # 迭代训练 / Iterative training\n",
    "        for _ in range(self.max_iter):\n",
    "            for _, row in train_data.iterrows():\n",
    "                u = user_map[row['userId']]\n",
    "                i = movie_map[row['movieId']]\n",
    "                y = year_map[row['year']]\n",
    "                genres = row[genre_cols].values.astype(float)\n",
    "                \n",
    "                # 预测评分 / Predict rating\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "                error = row['rating'] - pred\n",
    "                \n",
    "                # 更新参数 / Update parameters\n",
    "                self.b_u[u] += self.alpha * (error - self.lambda_reg * self.b_u[u])\n",
    "                self.b_i[i] += self.alpha * (error - self.lambda_reg * self.b_i[i])\n",
    "                self.P[u] += self.alpha * (error * self.Q[i] - self.lambda_reg * self.P[u])\n",
    "                self.Q[i] += self.alpha * (error * self.P[u] - self.lambda_reg * self.Q[i])\n",
    "                self.b_g += self.alpha * (error * genres - self.lambda_reg * self.b_g)\n",
    "                self.b_y[y] += self.alpha * (error - self.lambda_reg * self.b_y[y])\n",
    "    \n",
    "    def predict(self, test_data, user_map, movie_map, year_map, genre_cols):\n",
    "        \"\"\"\n",
    "        中文：预测测试集评分\n",
    "        English: Predict ratings for the test set\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in test_data.iterrows():\n",
    "            u = user_map.get(row['userId'], -1)  # 如果用户不在训练集中，返回 -1 / Return -1 if user not in training set\n",
    "            i = movie_map.get(row['movieId'], -1)  # 如果电影不在训练集中，返回 -1 / Return -1 if movie not in training set\n",
    "            y = year_map.get(row['year'], -1)  # 如果年份不在训练集中，返回 -1 / Return -1 if year not in training set\n",
    "            genres = row[genre_cols].values.astype(float)\n",
    "            \n",
    "            # 冷启动处理 / Cold start handling\n",
    "            if u == -1 or i == -1 or y == -1:\n",
    "                pred = self.mu  # 对于新用户或新电影，使用全局均值 / Use global mean for new users or movies\n",
    "            else:\n",
    "                pred = (self.mu + self.b_u[u] + self.b_i[i] + \n",
    "                        np.dot(self.P[u], self.Q[i]) + \n",
    "                        np.dot(self.b_g, genres) + self.b_y[y])\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 后处理预测评分 / Post-process predicted ratings\n",
    "def postprocess_predictions(predictions):\n",
    "    \"\"\"\n",
    "    中文：将预测评分裁剪到 0-5 并四舍五入到最近的 0.5 倍数。\n",
    "    English: Clip predictions to 0-5 and round to the nearest 0.5 multiple.\n",
    "    :param predictions: 原始预测评分 / Raw predicted ratings\n",
    "    :return: 处理后的预测评分 / Processed predicted ratings\n",
    "    \"\"\"\n",
    "    # 裁剪到 0-5 / Clip to 0-5\n",
    "    clipped_predictions = np.clip(predictions, 0, 5)\n",
    "    \n",
    "    # 四舍五入到最近的 0.5 倍数 / Round to nearest 0.5 multiple\n",
    "    rounded_predictions = np.round(clipped_predictions * 2) / 2\n",
    "    return rounded_predictions\n",
    "\n",
    "# 主程序 / Main function\n",
    "def main():\n",
    "    # 预处理训练集 / Preprocess training set\n",
    "    train_data_processed, mlb = preprocess_data(train_data, is_train=True)\n",
    "    \n",
    "    # 创建映射字典 / Create mapping dictionaries\n",
    "    user_ids = train_data_processed['userId'].unique()\n",
    "    movie_ids = train_data_processed['movieId'].unique()\n",
    "    years = train_data_processed['year'].unique()\n",
    "    genre_cols = mlb.classes_\n",
    "    \n",
    "    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "    year_map = {y: idx for idx, y in enumerate(years)}\n",
    "    \n",
    "    # 初始化并训练模型 / Initialize and train the model\n",
    "    model = SGDRecommender(\n",
    "        num_users=len(user_ids),\n",
    "        num_movies=len(movie_ids),\n",
    "        num_genres=len(genre_cols),\n",
    "        num_years=len(years),\n",
    "        k=20,         # 潜在特征维度 / Latent feature dimension\n",
    "        alpha=0.005,  # 学习率 / Learning rate\n",
    "        lambda_reg=0.1,  # 正则化参数 / Regularization parameter\n",
    "        max_iter=100    # 迭代次数 / Number of iterations\n",
    "    )\n",
    "    print(\"开始训练模型... / Starting model training...\")\n",
    "    model.fit(train_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 预处理测试集 / Preprocess test set\n",
    "    test_data_processed, _ = preprocess_data(test_data, mlb=mlb, is_train=False)\n",
    "    \n",
    "    # 预测测试集评分 / Predict ratings for test set\n",
    "    print(\"开始预测测试集... / Starting prediction on test set...\")\n",
    "    raw_predictions = model.predict(test_data_processed, user_map, movie_map, year_map, genre_cols)\n",
    "    \n",
    "    # 后处理预测评分 / Post-process predictions\n",
    "    final_predictions = postprocess_predictions(raw_predictions)\n",
    "    \n",
    "    # 将处理后的预测结果添加到测试集 / Add processed predictions to test set\n",
    "    test_data_processed['predicted_rating'] = final_predictions\n",
    "    \n",
    "    # 输出预测结果 / Output prediction results\n",
    "    print(\"预测结果示例： / Prediction examples:\")\n",
    "    print(test_data_processed[['userId', 'movieId', 'timestamp', 'predicted_rating']].head())\n",
    "    \n",
    "    # 保存预测结果到文件 / Save predictions to file\n",
    "    test_data_processed.to_csv('predicted_ratings.csv', index=False)\n",
    "    print(\"预测结果已保存到 'predicted_ratings.csv' / Predictions saved to 'predicted_ratings.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
